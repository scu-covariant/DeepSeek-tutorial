{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeek GRPO 训练流程\n",
    "在本文件中，我们将手动实现一个 GRPO 并使用它为 Qwen2.5 模型添加类似于 DeepSeek 的推理能力，旨在了解 GRPO 的技术原理并掌握其基本实现方法。\n",
    "\n",
    "## 1 初始化模型并构建聊天模板\n",
    "### 1.1 安装所需依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: modelscope in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (1.23.1)\n",
      "Requirement already satisfied: requests>=2.25 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from modelscope) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from modelscope) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.26 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from modelscope) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from requests>=2.25->modelscope) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from requests>=2.25->modelscope) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from requests>=2.25->modelscope) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: trl in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (0.15.2)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from trl) (1.4.0)\n",
      "Requirement already satisfied: datasets>=2.21.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from trl) (3.3.2)\n",
      "Requirement already satisfied: rich in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from trl) (13.9.4)\n",
      "Requirement already satisfied: transformers>=4.46.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from trl) (4.49.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (0.29.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (0.5.3)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.21.0->trl) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (3.11.13)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from transformers>=4.46.0->trl) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from transformers>=4.46.0->trl) (0.21.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from rich->trl) (2.19.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.12.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from pandas->datasets>=2.21.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from pandas->datasets>=2.21.0->trl) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from pandas->datasets>=2.21.0->trl) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install modelscope\n",
    "%pip install torch\n",
    "%pip install transformers\n",
    "%pip install trl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 下载模型\n",
    "为了尽可能减少训练成本，我们使用参数量为 0.5B 的 Qwen2.5 模型作为基座模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 在本地创建文件夹用于保存模型参数\n",
    "os.makedirs(\"./Qwen2.5-0.5B-Instruct\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: /Users/bihan/Projects/Deepseek_tutorial/Qwen2.5-0.5B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# 从 modelscope 库下载 Qwen2.5 模型到指定文件夹\n",
    "!modelscope download --model Qwen/Qwen2.5-0.5B-Instruct --local_dir ./Qwen2.5-0.5B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 加载模型与分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 指定模型所在目录\n",
    "model_name = \"./Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AutoModelForCausalLM.from_pretrained()`:\n",
    "\n",
    "**核心功能：**\n",
    "\n",
    "- 从指定路径加载模型\n",
    "- 支持动态选择计算精度并分配设备\n",
    "\n",
    "**参数解析：**\n",
    "\n",
    "- `pretrained_model_name_or_path`: 基座模型路径。\n",
    "- `torch_dtype`: 选择张量数据类型。\"auto\"表示根据硬件能力自动选择张量数据类型(float32, float16, bfloat16)，在保持精度的同时最大化计算效率。\n",
    "- `device_map`: 选择设备分配策略。\"auto\"表示根据硬件能力自动分配 CPU/GPU 资源。\n",
    "\n",
    "`AutoTokenizer.from_pretrained()`:\n",
    "\n",
    "**核心功能：**\n",
    "\n",
    "- 从指定路径加载文件的 tokenizer(分词器)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 创建对话消息\n",
    "平时我们在使用大语言模型应用进行对话时，在聊天框中输入的内容就称为 prompt。输入的 prompt 在被传输到后台服务器时，会被组织成`{\"role\": \"user\", \"content\": <你输入的 prompt>}`这样的形式（注：这种数据类型叫做`json`，大家之后在数据处理任务中会经常使用到它），用于表示这是用户发送的消息。\n",
    "\n",
    "通常而言，在聊天这一应用场景中，每条消息均以 json 格式定义，包含两个关键字段：\n",
    "\n",
    "- `role`: 标识消息来源，支持三种标准角色：\n",
    "  - `user`: 用户发送的消息（输入的 prompt）\n",
    "  - `assistant`: 模型生成的回复\n",
    "  - `system`: 系统级指令（用于预设模型行为，通常仅出现在首条消息）\n",
    "- `content`: 消息的文本内容\n",
    "\n",
    "因此，用户和系统之间的消息你来我往，便能够构成如下所示的消息历史：\n",
    "\n",
    "```json\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"......\"},\n",
    "  {\"role\": \"user\", \"content\": \"......\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"......\"},\n",
    "  {\"role\": \"user\", \"content\": \"......\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"......\"},\n",
    "  ......\n",
    "]\n",
    "```\n",
    "\n",
    "每一次回答时，消息历史中的所有内容都会被作为模型输入，因此你会发现模型总能够“记住”你们之间说过的话（实际上，模型输入有一定的长度限制，即“上下文窗口长度”，通常按照 token 数量来计算，比如 4K/16K tokens，超出长度的部分会被自动截断）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 原始消息结构 =====\n",
      " [{'role': 'user', 'content': 'Joy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?'}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Joy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "print(\"\\n===== 原始消息结构 =====\\n\", messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面说到，模型和用户的对话历史会被组织成 json 列表的形式传输给模型，接下来便需要模型对上述消息作出回答。\n",
    "\n",
    "我们知道，语言模型所做的任务本质上就是“文字接龙”，而聊天是一项比较特殊的“文字接龙”，因为它加入了身份的切换，从而让输出看起来像是一场对话。因此同其他任务一样，在聊天这项任务中，模型在作出回答时，首先也需要先做 tokenize 操作，也就是将大段的文字切分成一个又一个模型认识的小字段，也就是 token。\n",
    "\n",
    "为了将`messages`格式化为模型输入，需要调用`tokenizer.apply_chat_template()`方法。并为该方法设定了两个参数值：`tokenize=False`和`add_generation_prompt=True`。\n",
    "\n",
    "怎么理解这两个参数呢？\n",
    "\n",
    "首先，`add_chat_template()`会将传入的 json 列表`messages`拼接为一段文本。接着，如果`tokenize=True`，则这一段文本会被直接被切分成 tokens，最终输出由各个 token 的 id 组成的数字序列；如果`tokenize=False`，则会保留文本原本的样式。具体区别可以对比下面代码块中的`text`和`text_1`。\n",
    "\n",
    "然后是`add_generation_prompt`这个参数。简单来说，如果`add_generation_prompt=True`，那么每次在`<|im_start|>user......<|im_end|>`后，都会自动接一个`<|im_start|>assistant`，用来告诉模型：“嘿，用户已经说完了，现在该你回答了”；如果`add_generation_prompt=False`，则不会自动接这个提示词。不过一般来讲，在实操中我们都会选择`add_generation_prompt=True`，就像上面所说的，我们是在用一个“文字接龙”的机器模拟“聊天”，因此如果不刻意地给模型一些提示，它可能会接着补充用户的内容，而不是“发表”自己的意见。具体区别可以对比下面代码块中的`text`和`text_2`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== text =====\n",
      " <|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Joy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "===== text_1 =====\n",
      " [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 79771, 646, 1349, 220, 23, 6816, 315, 264, 2311, 304, 220, 17, 15, 4420, 13, 2585, 1657, 4115, 686, 432, 1896, 1059, 311, 1349, 220, 16, 17, 15, 6816, 30, 151645, 198, 151644, 77091, 198]\n",
      "\n",
      "===== text_2 =====\n",
      " <|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Joy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "text_1 = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "text_2 = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False\n",
    ")\n",
    "\n",
    "print(\"\\n===== text =====\\n\", text)\n",
    "print(\"\\n===== text_1 =====\\n\", text_1)\n",
    "print(\"\\n===== text_2 =====\\n\", text_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 将文本格式化为模型输入\n",
    "截至目前我们已经获得了经过`apply_chat_template`格式化后的`text`，接下来需要将文本格式化为模型能够理解的输入，这一过程需要使用的是`tokenizer`，最终返回的则是一个列表，列表由每个 token 在词汇表中对应的 id 组成，也就是代码块输出的`input_ids`。\n",
    "\n",
    "此外，输出中还包含`attention_mask`和`device`，其中`attention_mask`用来标识哪些位置是有效 token（1=有效，0=填充位）；`device`将张量移动到模型所在设备，因为模型参数和张量必须在同一设备上才能计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "             13, 151645,    198, 151644,    872,    198,  79771,    646,   1349,\n",
      "            220,     23,   6816,    315,    264,   2311,    304,    220,     17,\n",
      "             15,   4420,     13,   2585,   1657,   4115,    686,    432,   1896,\n",
      "           1059,    311,   1349,    220,     16,     17,     15,   6816,     30,\n",
      "         151645,    198, 151644,  77091,    198]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='mps:0')}\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer(\n",
    "    [text],                # 输入文本（列表形式，支持批量输入）\n",
    "    return_tensors=\"pt\"    # 返回 PyTorch 张量\n",
    "    ).to(model.device)     # 将张量迁移到模型所在设备\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 获取模型输出并转化为文本\n",
    "接下来我们将输入喂给模型并获取模型输出，第一步先是获取模型新生成的 token 的 id 序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([  1249,   8253,   1246,   1293,    432,    686,   1896,  27138,    311,\n",
      "          1349,    220,     16,     17,     15,   6816,     11,    582,   1156,\n",
      "          1184,    311,   1477,    700,   1246,   1293,   1340,   4990,    311,\n",
      "          1349,    825,   2150,    323,   1221,    990,    429,   1995,    311,\n",
      "         11047,    279,   2790,    882,    369,    220,     16,     17,     15,\n",
      "          6816,    382,     16,     13,  20517,    279,   5290,   4628,    304,\n",
      "          6816,    817,   9383,    510,    256,   1124,   9640,    256,   1124,\n",
      "          1318,     90,  31899,   4628,     92,    284,   1124,  37018,     90,\n",
      "            23,   1124,   1318,     90,   6816,   3417,     90,     17,     15,\n",
      "          1124,   1318,     90,   4420,   3417,    284,    220,     15,     13,\n",
      "            19,   1124,   1318,     90,   6816,    817,   9383,    532,    256,\n",
      "          1124,   2533,     17,     13,  29901,    279,    882,    432,   4990,\n",
      "         27138,    311,   1349,    220,     16,     17,     15,   6816,    518,\n",
      "           419,   4379,    510,    256,   1124,   9640,    256,   1124,   1318,\n",
      "            90,   1462,    311,   1349,    220,     16,     17,     15,   6816,\n",
      "            92,    284,   1124,  37018,     90,     16,     17,     15,   1124,\n",
      "          1318,     90,   6816,   3417,     90,     15,     13,     19,   1124,\n",
      "          1318,     90,   6816,    817,   9383,   3417,    284,    220,     18,\n",
      "            15,     15,   1124,   1318,     90,   4420,    532,    256,   1124,\n",
      "          2533,     18,     13,   7169,    279,    882,    504,   4420,    311,\n",
      "          4115,    510,    256,   1124,   9640,    256,   1124,   1318,     90,\n",
      "          1462,    304,   4115,     92,    284,   1124,  37018,     90,     18,\n",
      "            15,     15,   1124,   1318,     90,   4420,   3417,     90,     21,\n",
      "            15,   1124,   1318,     90,   4420,    817,   6460,   3417,    284,\n",
      "           220,     20,   1124,   1318,     90,   4115,    532,    256,   1124,\n",
      "          2533,  54815,     11,    432,    686,   1896,  27138,   1124,  79075,\n",
      "            90,     20,     92,   4115,    311,   1349,    220,     16,     17,\n",
      "            15,   6816,     13, 151645], device='mps:0')]\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512    # 限制模型生成新的 token 数不超过 512\n",
    ")\n",
    "# 遍历每个输入样本的 input_ids 和对应的生成结果 output_ids\n",
    "# 对每个样本，计算输入长度 len(input_ids)，截取生成结果中从该位置到末尾的部分\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):]\n",
    "    for input_ids, output_ids\n",
    "    in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "print(generated_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后将 id 转化为对应的 token 文本，最终组合成一个字符串，即模型的回答。\n",
    "\n",
    "`skip_special_tokens=True`表明跳过特殊标记如`<|im_start|>`,`<|im_end|>`等，这是因为特殊标记虽然对模型有意义，但对用户无意义，所以通常需要跳过。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine how long it will take Joy to read 120 pages, we first need to find out how long she takes to read one page and then use that information to calculate the total time for 120 pages.\n",
      "\n",
      "1. Calculate the reading speed in pages per minute:\n",
      "   \\[\n",
      "   \\text{Reading speed} = \\frac{8 \\text{ pages}}{20 \\text{ minutes}} = 0.4 \\text{ pages per minute}\n",
      "   \\]\n",
      "\n",
      "2. Determine the time it takes Joy to read 120 pages at this rate:\n",
      "   \\[\n",
      "   \\text{Time to read 120 pages} = \\frac{120 \\text{ pages}}{0.4 \\text{ pages per minute}} = 300 \\text{ minutes}\n",
      "   \\]\n",
      "\n",
      "3. Convert the time from minutes to hours:\n",
      "   \\[\n",
      "   \\text{Time in hours} = \\frac{300 \\text{ minutes}}{60 \\text{ minutes per hour}} = 5 \\text{ hours}\n",
      "   \\]\n",
      "\n",
      "Therefore, it will take Joy \\boxed{5} hours to read 120 pages.\n"
     ]
    }
   ],
   "source": [
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们成功用原生的 Qwen2.5 大模型构建起了一个能够聊天的机器人，但是目前它的回答是糅合在一起的，并不具备像 DeepSeek 那样的推理能力，接下来我们就为它添加推理能力。\n",
    "## 2 尝试用 prompt 注入方式为模型添加推理能力\n",
    "说起为模型添加推理能力，我们首先能够想到的便是通过 prompt 提示模型先给出推理然后再给出答案，那么这种简单朴素的想法是否能够成功呢？让我们一探究竟。\n",
    "\n",
    "这部分需要用到一个名为 gsm8k 的数据集，它是一个广泛用于评估语言模型数学推理能力的基准数据集，包含 8500 个高质量的小学数学应用题，每条数据均包含`question`和`answer`两项内容，且`answer`中不仅有问题的答案，还包含了问题的计算推导过程，因此该数据集常用来测试模型理解和处理数学推理能力。该数据集可以直接从`datasets`库中获取。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autodl 添加学术加速\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== gsm8k 数据信息 =====\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 7473\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1319\n",
      "    })\n",
      "}) \n",
      "\n",
      "\n",
      "===== gsm8k 训练集数据展示（第一条） =====\n",
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n"
     ]
    }
   ],
   "source": [
    "import datasets as hf_datasets\n",
    "data = hf_datasets.load_dataset('openai/gsm8k', 'main')\n",
    "print(\"\\n===== gsm8k 数据信息 =====\")\n",
    "print(data,\"\\n\")\n",
    "print(\"\\n===== gsm8k 训练集数据展示（第一条） =====\")\n",
    "print(data['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仔细观察可以看到，`answer`被`####`分成了两部分，前半部分是`reasoning`，后半部分则是直接给出`answer`。现在你可能已经想象到我们最终要赋予给 Qwen2.5 的是一个怎样的推理能力了。\n",
    "\n",
    "对！就是要让它在`reasoning`时给出细致的数学推理过程，在回答时直接给出`answer`。\n",
    "\n",
    "![image-20250305195837974](https://asdfdasgasd.oss-cn-chengdu.aliyuncs.com/typora_pictures/20250305195838053.png)\n",
    "\n",
    "接下来对原始的 gsm8k 数据集进行重构，剔除`answer`中的`reasoning`部分，修改后的结构如下所示：\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"question\": \"对应原数据集中的 question\",\n",
    "  \"answer\": \"仅包含原 answer 中 #### 之后的部分\",\n",
    "  \"prompt\":\n",
    "  \t[\n",
    "  \t\t{\n",
    "  \t\t\t\"role\": \"system\",\n",
    "  \t\t\t\"content\": \"SYSTEM_PROMPT，旨在提示模型按照先给出 reasoning 再给出 answer 的格式回答\",\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"原数据集中的 question\"\n",
    "      }\n",
    "  \t]\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    \"\"\"  \n",
    "    从原始文本中提取 answer(#### 之后的部分)\n",
    "    \"\"\"\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split]\n",
    "    data = data.map(lambda x: {\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': x['question']}\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 重构后的数据集信息 =====\n",
      "Dataset({\n",
      "    features: ['question', 'answer', 'prompt'],\n",
      "    num_rows: 7473\n",
      "})\n",
      "\n",
      "===== 重构后的第一条数据 =====\n",
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': '72', 'prompt': [{'content': '\\nRespond in the following format:\\n<reasoning>\\n...\\n</reasoning>\\n<answer>\\n...\\n</answer>\\n', 'role': 'system'}, {'content': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'role': 'user'}]}\n"
     ]
    }
   ],
   "source": [
    "dataset = get_gsm8k_questions()\n",
    "print(\"\\n===== 重构后的数据集信息 =====\")\n",
    "print(dataset)\n",
    "print(\"\\n===== 重构后的第一条数据 =====\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "细心的你可能已经发现了，重构之后的数据集中，键`prompt`对应的值本身就是一个对话历史，因此我们可以直接把`prompt`取出来作为`messages`喂给大模型，并获取大模型的回答。而且这次，咱们的`prompt`中包含了`SYSTEM_PROMPT`，提示模型先给出推理再给出答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine the total number of clips Natalia sold in April and May, we need to follow these steps:\n",
      "\n",
      "1. **Identify the number of clips sold in April:**\n",
      "   Natalia sold clips to 48 friends in April.\n",
      "\n",
      "2. **Calculate the number of clips sold in May:**\n",
      "   In May, Natalia sold half as many clips as she did in April.\n",
      "   \\[\n",
      "   \\text{Number of clips sold in May} = \\frac{\\text{Number of clips sold in April}}{2}\n",
      "   \\]\n",
      "   Substituting the number of clips sold in April:\n",
      "   \\[\n",
      "   \\text{Number of clips sold in May} = \\frac{48}{2} = 24\n",
      "   \\]\n",
      "\n",
      "3. **Calculate the total number of clips sold in both months:**\n",
      "   Add the number of clips sold in April to the number of clips sold in May.\n",
      "   \\[\n",
      "   \\text{Total number of clips sold} = \\text{Number of clips sold in April} + \\text{Number of clips sold in May}\n",
      "   \\]\n",
      "   Substituting the values:\n",
      "   \\[\n",
      "   \\text{Total number of clips sold} = 48 + 24 = 72\n",
      "   \\]\n",
      "\n",
      "Thus, the total number of clips Natalia sold altogether in April and May is \\boxed{72}.\n"
     ]
    }
   ],
   "source": [
    "messages = dataset[0]['prompt']\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而结果并不理想，模型并没有诞生思考过程，且结果也没有按照我们限定的格式返回，看来只靠 prompt 这样的“口头要求”来约束模型回答还是远远不够的。\n",
    "\n",
    "专业的事交给专业的“人”，GRPO 这项技术就是专门用来训练模型逻辑推理能力的一种强化学习框架，这项技术由 DeepSeek 团队于 2024 年 4 月在 DeepSeekMath 这篇文章中首次提出，由于本篇教学仅做简单科普，因此关于其数学原理我们暂不讨论，有兴趣的读者可参考 [原论文](https://arxiv.org/abs/2402.03300) 并深入学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 GRPO 奖励函数组复现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先前我们已经将 gsm8k 数据集进行了重组，重组之后每条`prompt`都可以作为对话历史喂给模型从而获得输出。GRPO 做的事情是，对每一条回答都会进行评估，在当前的数学计算场景下，有两项评估指标，分别是“准确性评估”和“格式评估”。如果回答结果准确，则给予模型“准确性奖励”，否则不给予奖励；如果回答格式准确，则给予模型“格式奖励”，否则不给予奖励。\n",
    "\n",
    "下面将一一实现需要的奖励函数，大都是硬性的文本匹配规则，可讲的不多，主要是解释代码的实现方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'To determine the total number of clips Natalia sold in April and May, we need to follow these steps:\\n\\n1. **Identify the number of clips sold in April:**\\n   Natalia sold clips to 48 friends in April.\\n\\n2. **Calculate the number of clips sold in May:**\\n   In May, Natalia sold half as many clips as she did in April.\\n   \\\\[\\n   \\\\text{Number of clips sold in May} = \\\\frac{\\\\text{Number of clips sold in April}}{2}\\n   \\\\]\\n   Substituting the number of clips sold in April:\\n   \\\\[\\n   \\\\text{Number of clips sold in May} = \\\\frac{48}{2} = 24\\n   \\\\]\\n\\n3. **Calculate the total number of clips sold in both months:**\\n   Add the number of clips sold in April to the number of clips sold in May.\\n   \\\\[\\n   \\\\text{Total number of clips sold} = \\\\text{Number of clips sold in April} + \\\\text{Number of clips sold in May}\\n   \\\\]\\n   Substituting the values:\\n   \\\\[\\n   \\\\text{Total number of clips sold} = 48 + 24 = 72\\n   \\\\]\\n\\nThus, the total number of clips Natalia sold altogether in April and May is \\\\boxed{72}.'}]\n"
     ]
    }
   ],
   "source": [
    "# 封装模型回答\n",
    "completion=  [{'content': response}]\n",
    "completions = [completion]\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml_answer(text: str) -> str:\n",
    "    \"\"\"  \n",
    "    从模型回答的文本中提取数学结果（<answer></answer>之间的内容）。\n",
    "    \"\"\"\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正确性检验\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    \"\"\"  \n",
    "    检查模型输出是否与正确答案相匹配，并根据匹配情况返回奖励分数。\n",
    "\n",
    "    输入：\n",
    "    prompts: 由数据集中各条数据的`prompt`组成的列表，即输入给模型的对话历史。\n",
    "    completions: 由模型回答组成的列表。\n",
    "    answer: 由数据集中各条数据的`answer`组成的列表，即问题的正确答案。\n",
    "\n",
    "    输出：\n",
    "    奖励分数列表。\n",
    "    \"\"\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content']\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    print('-'*20, \" Question \", '-'*20, \"\\n\", q)\n",
    "    print('-'*20, \" Answer \", '-'*20, \"\\n\", answer)\n",
    "    print('-'*20, \" Response \", '-'*20, \"\\n\", responses[0])\n",
    "    print('-'*20, \" Extracted \", '-'*20, \"\\n\", extracted_responses[0])\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现：\n",
    "\n",
    "- `responses = [completion[0]['content'] for completion in completions]`: 提取每个 competion 中的内容（即模型的输出）\n",
    "- `q = prompts[0][-1]['content']`: 提取输入问题（prompt）的内容\n",
    "- `extracted_responses = [extract_xml_answer(r) for r in responses]`: 用`extract_xml_answer()`方法从模型输出中提取出答案部分\n",
    "- `print(...)`打印问题、正确答案、模型输出和提取的答案\n",
    "- `return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]`: 比较提取出来的每个答案与正确答案。如果两者相同，则返回奖励 2.0；否则返回 0.0。奖励是一个列表，长度与模型的输出数目相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------  Quention  -------------------- \n",
      " Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "--------------------  Answer  -------------------- \n",
      " 72\n",
      "--------------------  Response  -------------------- \n",
      " To determine the total number of clips Natalia sold in April and May, we need to follow these steps:\n",
      "\n",
      "1. **Identify the number of clips sold in April:**\n",
      "   Natalia sold clips to 48 friends in April.\n",
      "\n",
      "2. **Calculate the number of clips sold in May:**\n",
      "   In May, Natalia sold half as many clips as she did in April.\n",
      "   \\[\n",
      "   \\text{Number of clips sold in May} = \\frac{\\text{Number of clips sold in April}}{2}\n",
      "   \\]\n",
      "   Substituting the number of clips sold in April:\n",
      "   \\[\n",
      "   \\text{Number of clips sold in May} = \\frac{48}{2} = 24\n",
      "   \\]\n",
      "\n",
      "3. **Calculate the total number of clips sold in both months:**\n",
      "   Add the number of clips sold in April to the number of clips sold in May.\n",
      "   \\[\n",
      "   \\text{Total number of clips sold} = \\text{Number of clips sold in April} + \\text{Number of clips sold in May}\n",
      "   \\]\n",
      "   Substituting the values:\n",
      "   \\[\n",
      "   \\text{Total number of clips sold} = 48 + 24 = 72\n",
      "   \\]\n",
      "\n",
      "Thus, the total number of clips Natalia sold altogether in April and May is \\boxed{72}.\n",
      "--------------------  Extracted  -------------------- \n",
      " To determine the total number of clips Natalia sold in April and May, we need to follow these steps:\n",
      "\n",
      "1. **Identify the number of clips sold in April:**\n",
      "   Natalia sold clips to 48 friends in April.\n",
      "\n",
      "2. **Calculate the number of clips sold in May:**\n",
      "   In May, Natalia sold half as many clips as she did in April.\n",
      "   \\[\n",
      "   \\text{Number of clips sold in May} = \\frac{\\text{Number of clips sold in April}}{2}\n",
      "   \\]\n",
      "   Substituting the number of clips sold in April:\n",
      "   \\[\n",
      "   \\text{Number of clips sold in May} = \\frac{48}{2} = 24\n",
      "   \\]\n",
      "\n",
      "3. **Calculate the total number of clips sold in both months:**\n",
      "   Add the number of clips sold in April to the number of clips sold in May.\n",
      "   \\[\n",
      "   \\text{Total number of clips sold} = \\text{Number of clips sold in April} + \\text{Number of clips sold in May}\n",
      "   \\]\n",
      "   Substituting the values:\n",
      "   \\[\n",
      "   \\text{Total number of clips sold} = 48 + 24 = 72\n",
      "   \\]\n",
      "\n",
      "Thus, the total number of clips Natalia sold altogether in April and May is \\boxed{72}.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctness_reward_func(prompts=[dataset[0]['prompt']],\n",
    "                        completions=completions,\n",
    "                        answer=dataset[0]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"  \n",
    "    检查模型输出是否为有效整数，并根据结果给予奖励。\n",
    "\n",
    "    输入：\n",
    "    completions: 由模型回答组成的列表。\n",
    "\n",
    "    输出：\n",
    "    奖励分数列表。\n",
    "    \"\"\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `responses = [completion[0]['content'] for completion in completions]`: 提取模型回答的内容\n",
    "- `extracted_responses = [extract_xml_answer(r) for r in responses]`: 从模型回答中提取答案\n",
    "- `return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]`: 检查回答的答案是否为整数形式。如果是则给予奖励 0.5；否则不给予奖励。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_reward_func(completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 强格式检验\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"  \n",
    "    检查模型输出是否符合严格的格式要求。\n",
    "\n",
    "    输入：\n",
    "    completions: 由模型回答组成的列表。\n",
    "\n",
    "    输出：\n",
    "    奖励分数列表。\n",
    "    \"\"\"\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ` pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"`: 正则表达式，确保输出格式严格符合要求\n",
    "- `responses = [completion[0][\"content\"] for completion in completions]`: 从模型回答中提取问题答案\n",
    "- `matches = [re.match(pattern, r) for r in responses]`: 通过正则表达式检查每个回答是否符合格式\n",
    "- `return [0.5 if match else 0.0 for match in matches]`: 如果格式匹配，则给予奖励 0.5；否则不给予奖励"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strict_format_reward_func(completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 弱格式检验\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"  \n",
    "    检查模型的输出是否符合稍微宽松的格式要求。\n",
    "\n",
    "    输入：\n",
    "    completions: 由模型回答组成的列表。\n",
    "\n",
    "    输出：\n",
    "    奖励分数列表。\n",
    "    \"\"\"\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"`: 定义了一个稍微宽松的表达式\n",
    "- `responses = [completion[0][\"content\"] for completion in completions]`: 从模型回答中提取问题答案\n",
    "- `matches = [re.match(pattern, r) for r in responses]`: 检查每个回答是否符合此格式\n",
    "- `return [0.5 if match else 0.0 for match in matches]`: 如果格式匹配则给予奖励 0.5；否则不给予奖励"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_format_reward_func(completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标签计数检验\n",
    "def count_xml(text) -> float:\n",
    "    \"\"\"  \n",
    "    计算文本中标签的出现次数，本根据他们的位置和频率分配奖励。\n",
    "\n",
    "    输入：\n",
    "    text: 计数文本。\n",
    "\n",
    "    输出：\n",
    "    count: 标签总分。\n",
    "    \"\"\"\n",
    "    count = 0.0\n",
    "    if text.count(\"<reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `count = 0.0`: 初始化计数器\n",
    "- `if text.count(\"<reasoning>\\n\") == 1:`: 检查是否有且仅有一个`<reasoning>\\n`标签\n",
    "- `count += 0.125`: 条件满足则奖励 0.125\n",
    "- `if text.count(\"\\n</reasoning>\\n\") == 1:`: 检查是否有且仅有一个`\\n<reasoning>`标签\n",
    "- `count += 0.125`: 条件满足则奖励 0.125\n",
    "- `if text.count(\"\\n<answer>\\n\") == 1:`: 检查是否有且仅有一个`\\n<answer>\\n`标签\n",
    "- `count += 0.125`: 条件满足则奖励 0.125\n",
    "- `count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001`: 如果在标签`\\n<answer>\\n`之后存在多余的文本，按照文本长度扣除一些奖励\n",
    "- `if text.count(\"\\n</answer>\") == 1:`: 检查是否有且仅有一个`\\n</answer>`标签\n",
    "- `count += 0.125`: 条件满足则奖励 0.125\n",
    "- `count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001`: 如果存在多余文本则根据其长度扣除一些奖励"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_xml(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结构符合度检验\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"  \n",
    "    计算每个模型输出的 XML 结构符合度，并给予奖励。\n",
    "\n",
    "    输入：\n",
    "    completions: 由模型回答组成的列表。\n",
    "\n",
    "    输出：\n",
    "    奖励分数列表。\n",
    "    \"\"\"\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [count_xml(c) for c in contents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `contents = [completion[0][\"content\"] for completion in completions]`: 从模型回答中提取答案\n",
    "- `return [count_xml(c) for c in contents]`: 把`count_xml()`输出的结果作为奖励分数并返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmlcount_reward_func(completions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 GRPO 训练流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 训练参数设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从指定位置读取模型\n",
    "model_name = \"./Qwen2.5-0.5B-Instruct\"\n",
    "# 指定模型训练中间值的输出地址\n",
    "output_dir=\"outputs/Qwen-0.5B-GRPO\"\n",
    "# 指定项目运行的名字\n",
    "run_name=\"Qwen-0.5B-GRPO-gsm8k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    run_name=run_name,\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_generations=16,\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=200,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=100,\n",
    "    max_grad_norm=0.1,\n",
    "    log_on_each_node=False,\n",
    "    use_vllm=False,\n",
    "    vllm_gpu_memory_utilization=.3,\n",
    "    vllm_device=\"cuda:0\",\n",
    "    report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 加载模型和分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=None\n",
    ").to(\"cuda\")\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 设置网页端监控"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"b45a51a2aa3d2984d2ac2089d28e9c6c94538a3a\")\n",
    "wandb.init(project=\"deepseek_tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 开始模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOTrainer\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        xmlcount_reward_func,\n",
    "        soft_format_reward_func,\n",
    "        strict_format_reward_func,\n",
    "        int_reward_func,\n",
    "        correctness_reward_func],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 每隔指定步将模型权重保存到指定文件夹\n",
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 运行模型并查看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpo_model_name = \"./outputs/Qwen-0.5B-GRPO\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    grpo_model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Joy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够看出，对比此前原始模型的状态，现在的模型已经能够顺利产生思考过程，且严格按照规定格式回答问题。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
