{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeek GRPO 训练流程\n",
    "在本文件中，我们将手动实现一个 GRPO 并使用它为 Qwen2.5 模型添加类似于 DeepSeek 的推理能力，旨在了解 GRPO 的技术原理并掌握其基本实现方法。\n",
    "\n",
    "> 文档的第 1～3 章的代码对机器要求不高，在本地即可运行起来；第 4 章代码是训练模型，对机器有要求，需要在 autodl 上租赁服务器。\n",
    ">\n",
    "> 因此如果你只是想简单看一遍了解一下，在本地即可；而如果你想从头到尾体验一遍 GRPO 训练过程而不想中间卡壳，请首先跳转至第 4.4 章节，了解需求并在 autodl 上租赁机器。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1 初始化模型并构建聊天模板\n",
    "### 1.1 安装所需依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: modelscope in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (1.23.1)\n",
      "Requirement already satisfied: requests>=2.25 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from modelscope) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from modelscope) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.26 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from modelscope) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from requests>=2.25->modelscope) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from requests>=2.25->modelscope) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from requests>=2.25->modelscope) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: trl in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (0.15.2)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from trl) (1.4.0)\n",
      "Requirement already satisfied: datasets>=2.21.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from trl) (3.3.2)\n",
      "Requirement already satisfied: rich in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from trl) (13.9.4)\n",
      "Requirement already satisfied: transformers>=4.46.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from trl) (4.49.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (0.29.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from accelerate>=0.34.0->trl) (0.5.3)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.21.0->trl) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from datasets>=2.21.0->trl) (3.11.13)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from transformers>=4.46.0->trl) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from transformers>=4.46.0->trl) (0.21.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from rich->trl) (2.19.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.12.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from pandas->datasets>=2.21.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from pandas->datasets>=2.21.0->trl) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from pandas->datasets>=2.21.0->trl) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/deepseek_tutorial/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install modelscope\n",
    "%pip install torch\n",
    "%pip install transformers\n",
    "%pip install trl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 下载模型\n",
    "为了尽可能减少训练成本，我们使用参数量为 0.5B 的 Qwen2.5 模型作为基座模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 在本地创建文件夹用于保存模型参数\n",
    "os.makedirs(\"./Qwen2.5-0.5B-Instruct\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /root/autodl-tmp/home/deepseek_tutorial/bin/modelscope: /home/deepseek_tutorial/bin/python: bad interpreter: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# 从 modelscope 库下载 Qwen2.5 模型到指定文件夹\n",
    "!modelscope download --model Qwen/Qwen2.5-0.5B-Instruct --local_dir ./Qwen2.5-0.5B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 加载模型与分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/home/deepseek_tutorial/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 指定模型所在目录\n",
    "model_name = \"./Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AutoModelForCausalLM.from_pretrained()`:\n",
    "\n",
    "**核心功能：**\n",
    "\n",
    "- 从指定路径加载模型\n",
    "- 支持动态选择计算精度并分配设备\n",
    "\n",
    "**参数解析：**\n",
    "\n",
    "- `pretrained_model_name_or_path`: 基座模型路径。\n",
    "- `torch_dtype`: 选择张量数据类型。\"auto\"表示根据硬件能力自动选择张量数据类型(float32, float16, bfloat16)，在保持精度的同时最大化计算效率。\n",
    "- `device_map`: 选择设备分配策略。\"auto\"表示根据硬件能力自动分配 CPU/GPU 资源。\n",
    "\n",
    "`AutoTokenizer.from_pretrained()`:\n",
    "\n",
    "**核心功能：**\n",
    "\n",
    "- 从指定路径加载文件的 tokenizer(分词器)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 创建对话消息\n",
    "平时我们在使用大语言模型应用进行对话时，在聊天框中输入的内容就称为 prompt。输入的 prompt 在被传输到后台服务器时，会被组织成`{\"role\": \"user\", \"content\": <你输入的 prompt>}`这样的形式（注：这种数据类型叫做`json`，大家之后在数据处理任务中会经常使用到它），用于表示这是用户发送的消息。\n",
    "\n",
    "通常而言，在聊天这一应用场景中，每条消息均以 json 格式定义，包含两个关键字段：\n",
    "\n",
    "- `role`: 标识消息来源，支持三种标准角色：\n",
    "  - `user`: 用户发送的消息（输入的 prompt）\n",
    "  - `assistant`: 模型生成的回复\n",
    "  - `system`: 系统级指令（用于预设模型行为，通常仅出现在首条消息）\n",
    "- `content`: 消息的文本内容\n",
    "\n",
    "因此，用户和系统之间的消息你来我往，便能够构成如下所示的消息历史：\n",
    "\n",
    "```json\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"......\"},\n",
    "  {\"role\": \"user\", \"content\": \"......\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"......\"},\n",
    "  {\"role\": \"user\", \"content\": \"......\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"......\"},\n",
    "  ......\n",
    "]\n",
    "```\n",
    "\n",
    "每一次回答时，消息历史中的所有内容都会被作为模型输入，因此你会发现模型总能够“记住”你们之间说过的话（实际上，模型输入有一定的长度限制，即“上下文窗口长度”，通常按照 token 数量来计算，比如 4K/16K tokens，超出长度的部分会被自动截断）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 原始消息结构 =====\n",
      " [{'role': 'user', 'content': 'Joy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?'}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Joy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "print(\"\\n===== 原始消息结构 =====\\n\", messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面说到，模型和用户的对话历史会被组织成 json 列表的形式传输给模型，接下来便需要模型对上述消息作出回答。\n",
    "\n",
    "我们知道，语言模型所做的任务本质上就是“文字接龙”，而聊天是一项比较特殊的“文字接龙”，因为它加入了身份的切换，从而让输出看起来像是一场对话。因此同其他任务一样，在聊天这项任务中，模型在作出回答时，首先也需要先做 tokenize 操作，也就是将大段的文字切分成一个又一个模型认识的小字段 token。\n",
    "\n",
    "为了将`messages`格式化为模型输入，需要调用`tokenizer.apply_chat_template()`方法。并为该方法设定了两个参数值：`tokenize=False`和`add_generation_prompt=True`。\n",
    "\n",
    "怎么理解这两个参数呢？\n",
    "\n",
    "首先，`add_chat_template()`会将传入的 json 列表`messages`拼接为一段文本。接着，如果`tokenize=True`，则这一段文本会被直接被切分成 tokens，最终输出由各个 token 的 id 组成的数字序列；如果`tokenize=False`，则会保留文本原本的样式。具体区别可以对比下面代码块中的`text`和`text_1`。\n",
    "\n",
    "然后是`add_generation_prompt`这个参数。简单来说，如果`add_generation_prompt=True`，那么每次在`<|im_start|>user......<|im_end|>`后，都会自动接一个`<|im_start|>assistant`，用来告诉模型：“嘿，用户已经说完了，现在该你回答了”；如果`add_generation_prompt=False`，则不会自动接这个提示词。\n",
    "\n",
    "不过一般来讲，在实操中我们都会选择`add_generation_prompt=True`，就像上面所说的，我们是在用一个“文字接龙”的机器模拟“聊天”，因此如果不刻意地给模型一些提示，它可能会接着补充用户的内容，而不是“发表”自己的意见。具体区别可以对比下面代码块中的`text`和`text_2`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== text =====\n",
      " <|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Joy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "===== text_1 =====\n",
      " [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 79771, 646, 1349, 220, 23, 6816, 315, 264, 2311, 304, 220, 17, 15, 4420, 13, 2585, 1657, 4115, 686, 432, 1896, 1059, 311, 1349, 220, 16, 17, 15, 6816, 30, 151645, 198, 151644, 77091, 198]\n",
      "\n",
      "===== text_2 =====\n",
      " <|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Joy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "text_1 = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "text_2 = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False\n",
    ")\n",
    "\n",
    "print(\"\\n===== text =====\\n\", text)\n",
    "print(\"\\n===== text_1 =====\\n\", text_1)\n",
    "print(\"\\n===== text_2 =====\\n\", text_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 将文本格式化为模型输入\n",
    "截至目前我们已经获得了经过`apply_chat_template`格式化后的`text`，接下来需要将文本格式化为模型能够理解的输入，这一过程需要使用的是`tokenizer`，最终返回的则是一个列表，列表由每个 token 在词汇表中对应的 id 组成，也就是代码块输出的`input_ids`。\n",
    "\n",
    "此外，输出中还包含`attention_mask`和`device`，其中`attention_mask`用来标识哪些位置是有效 token（1=有效，0=填充位）；`device`将张量移动到模型所在设备，因为模型参数和张量必须在同一设备上才能计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "             13, 151645,    198, 151644,    872,    198,  79771,    646,   1349,\n",
      "            220,     23,   6816,    315,    264,   2311,    304,    220,     17,\n",
      "             15,   4420,     13,   2585,   1657,   4115,    686,    432,   1896,\n",
      "           1059,    311,   1349,    220,     16,     17,     15,   6816,     30,\n",
      "         151645,    198, 151644,  77091,    198]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer(\n",
    "    [text],                # 输入文本（列表形式，支持批量输入）\n",
    "    return_tensors=\"pt\"    # 返回 PyTorch 张量\n",
    "    ).to(model.device)     # 将张量迁移到模型所在设备\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 获取模型输出并转化为文本\n",
    "接下来我们将输入喂给模型并获取模型输出，第一步先是获取模型新生成的 token 的 id 序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([  1249,   8253,   1246,   1293,    432,    686,   1896,  27138,    311,\n",
      "          1349,    220,     16,     17,     15,   6816,     11,    582,   1184,\n",
      "           311,   1156,   1477,    700,   1246,   1293,   1340,  37102,   5290,\n",
      "           825,   2150,    323,   1221,    990,    429,   4379,    311,  11047,\n",
      "           279,    882,    369,    220,     16,     17,     15,   6816,    382,\n",
      "            16,     13,   3070,  47866,    279,   5290,   4628,     25,   1019,\n",
      "           256,  27138,  15804,    220,     23,   6816,    304,    220,     17,\n",
      "            15,   4420,     13,  15277,     11,   1059,   5290,   4628,    374,\n",
      "           510,    256,   1124,   9640,    256,   1124,   1318,     90,  31899,\n",
      "          4628,     92,    284,   1124,  37018,     90,     23,   1124,   1318,\n",
      "            90,   6816,   3417,     90,     17,     15,   1124,   1318,     90,\n",
      "          4420,   3417,    284,   1124,  37018,     90,     23,  15170,     17,\n",
      "            15,     92,   1124,   1318,     90,   6816,    817,   9383,     92,\n",
      "           284,   1124,  37018,     90,     17,  15170,     20,     92,   1124,\n",
      "          1318,     90,   6816,    817,   9383,    532,    256,   1124,   2533,\n",
      "            17,     13,   3070,     35,  24308,    279,    882,    311,   1349,\n",
      "           220,     16,     17,     15,   6816,     25,   1019,    256,   1416,\n",
      "         27138,    594,   5290,   4628,    374,   1124,  11520,  37018,     90,\n",
      "            17,  15170,     20,  11035,      8,   6816,    817,   9383,     11,\n",
      "          1221,    279,    882,  17767,     51,  57758,    311,   1349,    220,\n",
      "            16,     17,     15,   6816,    374,   2661,    553,    510,    256,\n",
      "          1124,   9640,    256,    350,    284,   1124,  37018,     90,     16,\n",
      "            17,     15,   1124,   1318,     90,   6816,   3417,  35702,  37018,\n",
      "            90,     17,  15170,     20,     92,   1124,   1318,     90,   6816,\n",
      "           817,   9383,   3417,    284,    220,     16,     17,     15,   1124,\n",
      "         15136,   1124,  37018,     90,     20,  15170,     17,     92,    284,\n",
      "           220,     21,     15,   1124,  15136,    220,     20,    284,    220,\n",
      "            18,     15,     15,   1124,   1318,     90,   4420,    532,    256,\n",
      "          1124,    921,    256,   8704,   1052,    525,    220,     21,     15,\n",
      "          4420,    304,    458,   6460,     11,    582,   5508,    279,    882,\n",
      "           504,   4420,    311,   4115,    510,    256,   1124,   9640,    256,\n",
      "           350,    284,   1124,  37018,     90,     18,     15,     15,   1124,\n",
      "          1318,     90,   4420,   3417,     90,     21,     15,   1124,   1318,\n",
      "            90,   4420,    817,   6460,   3417,    284,    220,     20,   1124,\n",
      "          1318,     90,   4115,    532,    256,   1124,   2533,  54815,     11,\n",
      "           432,    686,   1896,  27138,   1124,  11520,  79075,     90,     20,\n",
      "         11035,      8,   4115,    311,   1349,    220,     16,     17,     15,\n",
      "          6816,     13, 151645], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512    # 限制模型生成新的 token 数不超过 512\n",
    ")\n",
    "# 遍历每个输入样本的 input_ids 和对应的生成结果 output_ids\n",
    "# 对每个样本，计算输入长度 len(input_ids)，截取生成结果中从该位置到末尾的部分\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):]\n",
    "    for input_ids, output_ids\n",
    "    in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "print(generated_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后将 id 转化为对应的 token 文本，最终组合成一个字符串，即模型的回答。\n",
    "\n",
    "`skip_special_tokens=True`表明跳过特殊标记如`<|im_start|>`,`<|im_end|>`等，这是因为特殊标记虽然对模型有意义，但对用户无意义，所以通常需要跳过。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine how long it will take Joy to read 120 pages, we need to first find out how long she spends reading one page and then use that rate to calculate the time for 120 pages.\n",
      "\n",
      "1. **Calculate the reading speed:**\n",
      "   Joy reads 8 pages in 20 minutes. Therefore, her reading speed is:\n",
      "   \\[\n",
      "   \\text{Reading speed} = \\frac{8 \\text{ pages}}{20 \\text{ minutes}} = \\frac{8}{20} \\text{ pages per minute} = \\frac{2}{5} \\text{ pages per minute}\n",
      "   \\]\n",
      "\n",
      "2. **Determine the time to read 120 pages:**\n",
      "   If Joy's reading speed is \\(\\frac{2}{5}\\) pages per minute, then the time \\(T\\) to read 120 pages is given by:\n",
      "   \\[\n",
      "   T = \\frac{120 \\text{ pages}}{\\frac{2}{5} \\text{ pages per minute}} = 120 \\times \\frac{5}{2} = 60 \\times 5 = 300 \\text{ minutes}\n",
      "   \\]\n",
      "   Since there are 60 minutes in an hour, we convert the time from minutes to hours:\n",
      "   \\[\n",
      "   T = \\frac{300 \\text{ minutes}}{60 \\text{ minutes per hour}} = 5 \\text{ hours}\n",
      "   \\]\n",
      "\n",
      "Therefore, it will take Joy \\(\\boxed{5}\\) hours to read 120 pages.\n"
     ]
    }
   ],
   "source": [
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们成功用原生的 Qwen2.5 大模型构建起了一个能够聊天的机器人，但是目前它的回答是糅合在一起的，并不具备像 DeepSeek 那样的推理能力，接下来我们就为它添加推理能力。\n",
    "## 2 尝试用 prompt 注入方式为模型添加推理能力\n",
    "说起为模型添加推理能力，我们首先能够想到的便是通过 prompt 提示模型先给出推理然后再给出答案，那么这种简单朴素的想法是否能够成功呢？让我们一探究竟。\n",
    "\n",
    "这部分需要用到一个名为 gsm8k 的数据集，它是一个广泛用于评估语言模型数学推理能力的基准数据集，包含 8500 个高质量的小学数学应用题，每条数据均包含`question`和`answer`两项内容，且`answer`中不仅有问题的答案，还包含了问题的计算推导过程，因此该数据集常用来测试模型理解和处理数学推理能力。该数据集可以直接从`datasets`库中获取。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autodl 添加学术加速，否则不能访问 huggingface 并下载数据集\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== gsm8k 数据信息 =====\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 7473\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1319\n",
      "    })\n",
      "}) \n",
      "\n",
      "\n",
      "===== gsm8k 训练集数据展示（第一条） =====\n",
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n"
     ]
    }
   ],
   "source": [
    "import datasets as hf_datasets\n",
    "data = hf_datasets.load_dataset('openai/gsm8k', 'main')\n",
    "print(\"\\n===== gsm8k 数据信息 =====\")\n",
    "print(data,\"\\n\")\n",
    "print(\"\\n===== gsm8k 训练集数据展示（第一条） =====\")\n",
    "print(data['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 关闭学术加速，否则会对正常网络造成影响\n",
    "!unset http_proxy && unset https_proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仔细观察可以看到，`answer`被`####`分成了两部分，前半部分是`reasoning`，后半部分则是直接给出`answer`。现在你可能已经想象到我们最终要赋予给 Qwen2.5 的是一个怎样的推理能力了。\n",
    "\n",
    "对！就是要让它在`reasoning`时给出细致的数学推理过程，在回答时直接给出`answer`。\n",
    "\n",
    "![image-20250305195837974](https://asdfdasgasd.oss-cn-chengdu.aliyuncs.com/typora_pictures/20250305195838053.png)\n",
    "\n",
    "接下来对原始的 gsm8k 数据集进行重构，剔除`answer`中的`reasoning`部分，修改后的结构如下所示：\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"question\": \"对应原数据集中的 question\",\n",
    "  \"answer\": \"仅包含原 answer 中 #### 之后的部分\",\n",
    "  \"prompt\":\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"SYSTEM_PROMPT，旨在提示模型按照先给出 reasoning 再给出 answer 的格式回答\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"原数据集中的 question\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    \"\"\"  \n",
    "    从原始文本中提取 answer(#### 之后的部分)\n",
    "    \"\"\"\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split]\n",
    "    data = data.map(lambda x: {\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': x['question']}\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 重构后的数据集信息 =====\n",
      "Dataset({\n",
      "    features: ['question', 'answer', 'prompt'],\n",
      "    num_rows: 7473\n",
      "})\n",
      "\n",
      "===== 重构后的第一条数据 =====\n",
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': '72', 'prompt': [{'content': '\\nRespond in the following format:\\n<reasoning>\\n...\\n</reasoning>\\n<answer>\\n...\\n</answer>\\n', 'role': 'system'}, {'content': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'role': 'user'}]}\n"
     ]
    }
   ],
   "source": [
    "dataset = get_gsm8k_questions()\n",
    "print(\"\\n===== 重构后的数据集信息 =====\")\n",
    "print(dataset)\n",
    "print(\"\\n===== 重构后的第一条数据 =====\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "细心的你可能已经发现了，重构之后的数据集中，键`prompt`对应的值本身就是一个对话历史，因此我们可以直接把`prompt`取出来作为`messages`喂给大模型，并获取大模型的回答。而且这次，咱们的`prompt`中包含了`SYSTEM_PROMPT`，提示模型先给出推理再给出答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find out how many clips Natalia sold altogether in April and May, we need to calculate the number of clips sold each month and then add them together.\n",
      "\n",
      "First, let's determine how many clips Natalia sold in May. Since she sold half as many clips in May as she did in April, we can calculate this by dividing the number of clips sold in April by 2:\n",
      "\n",
      "May sales = (Number of clips sold in April) / 2 = (48 clips) / 2 = 24 clips\n",
      "\n",
      "Now that we know the number of clips sold in both months, we can add them up to find the total sales:\n",
      "\n",
      "Total sales = Number of clips sold in April + Number of clips sold in May = 48 + 24 = 72 clips\n",
      "\n",
      "Therefore, Natalia sold a total of 72 clips altogether in April and May.\n"
     ]
    }
   ],
   "source": [
    "messages = dataset[0]['prompt']\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而结果并不理想，模型并没有诞生思考过程，且结果也没有按照我们限定的格式返回，看来只靠 prompt 这样的“口头要求”来约束模型回答还是远远不够的。\n",
    "\n",
    "专业的事交给专业的“人”，GRPO 这项技术就是专门用来训练模型逻辑推理能力的一种强化学习框架，这项技术由 DeepSeek 团队于 2024 年 4 月在 DeepSeekMath 这篇文章中首次提出，由于本篇教学仅做简单科普，因此关于其数学原理我们暂不讨论，有兴趣的读者可参考 [原论文](https://arxiv.org/abs/2402.03300) 并深入学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 GRPO 奖励函数组复现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先前我们已经将 gsm8k 数据集进行了重组，重组之后每条`prompt`都可以作为对话历史喂给模型从而获得输出。GRPO 做的事情是，对每一条回答都会进行评估，在当前的数学计算场景下，有两项评估指标，分别是“准确性评估”和“格式评估”。如果回答结果准确，则给予模型“准确性奖励”，否则不给予奖励；如果回答格式准确，则给予模型“格式奖励”，否则不给予奖励。\n",
    "\n",
    "下面将一一实现需要的奖励函数，大都是硬性的文本匹配规则，可讲的不多，主要是解释代码的实现方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': \"To find out how many clips Natalia sold altogether in April and May, we need to calculate the number of clips sold each month and then add them together.\\n\\nFirst, let's determine how many clips Natalia sold in May. Since she sold half as many clips in May as she did in April, we can calculate this by dividing the number of clips sold in April by 2:\\n\\nMay sales = (Number of clips sold in April) / 2 = (48 clips) / 2 = 24 clips\\n\\nNow that we know the number of clips sold in both months, we can add them up to find the total sales:\\n\\nTotal sales = Number of clips sold in April + Number of clips sold in May = 48 + 24 = 72 clips\\n\\nTherefore, Natalia sold a total of 72 clips altogether in April and May.\"}]\n"
     ]
    }
   ],
   "source": [
    "# 封装模型回答\n",
    "completion=  [{'content': response}]\n",
    "completions = [completion]\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml_answer(text: str) -> str:\n",
    "    \"\"\"  \n",
    "    从模型回答的文本中提取数学结果（<answer></answer>之间的内容）。\n",
    "    如果标签缺失, answer 将与 text 相同，即返回完整的原文本。\n",
    "    \"\"\"\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正确性检验\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    \"\"\"  \n",
    "    检查模型输出是否与正确答案相匹配，并根据匹配情况返回奖励分数。\n",
    "\n",
    "    输入：\n",
    "    prompts: 由数据集中各条数据的`prompt`组成的列表，即输入给模型的对话历史。\n",
    "    completions: 由模型回答组成的列表。\n",
    "    answer: 由数据集中各条数据的`answer`组成的列表，即问题的正确答案。\n",
    "\n",
    "    输出：\n",
    "    奖励分数列表。\n",
    "    \"\"\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content']\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    print(\"========================================================\")\n",
    "    print('-'*20, \" Question \", '-'*20, \"\\n\", q)\n",
    "    print('-'*20, \" Answer \", '-'*20, \"\\n\", answer)\n",
    "    print('-'*20, \" Response \", '-'*20, \"\\n\", responses[0])\n",
    "    print('-'*20, \" Extracted \", '-'*20, \"\\n\", extracted_responses[0])\n",
    "    print(\"========================================================\")\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `responses = [completion[0]['content'] for completion in completions]`: 提取每个 competion 中的内容（即模型的输出）\n",
    "- `q = prompts[0][-1]['content']`: 提取输入问题（prompt）的内容\n",
    "- `extracted_responses = [extract_xml_answer(r) for r in responses]`: 用`extract_xml_answer()`方法从模型输出中提取出答案部分\n",
    "- `print(...)`打印问题、正确答案、模型输出和提取的答案\n",
    "- `return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]`: 比较提取出来的每个答案与正确答案。如果两者相同，则返回奖励 2.0；否则返回 0.0。奖励是一个列表，长度与模型的输出数目相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "--------------------  Question  -------------------- \n",
      " Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "--------------------  Answer  -------------------- \n",
      " 72\n",
      "--------------------  Response  -------------------- \n",
      " To find out how many clips Natalia sold altogether in April and May, we need to calculate the number of clips sold each month and then add them together.\n",
      "\n",
      "First, let's determine how many clips Natalia sold in May. Since she sold half as many clips in May as she did in April, we can calculate this by dividing the number of clips sold in April by 2:\n",
      "\n",
      "May sales = (Number of clips sold in April) / 2 = (48 clips) / 2 = 24 clips\n",
      "\n",
      "Now that we know the number of clips sold in both months, we can add them up to find the total sales:\n",
      "\n",
      "Total sales = Number of clips sold in April + Number of clips sold in May = 48 + 24 = 72 clips\n",
      "\n",
      "Therefore, Natalia sold a total of 72 clips altogether in April and May.\n",
      "--------------------  Extracted  -------------------- \n",
      " To find out how many clips Natalia sold altogether in April and May, we need to calculate the number of clips sold each month and then add them together.\n",
      "\n",
      "First, let's determine how many clips Natalia sold in May. Since she sold half as many clips in May as she did in April, we can calculate this by dividing the number of clips sold in April by 2:\n",
      "\n",
      "May sales = (Number of clips sold in April) / 2 = (48 clips) / 2 = 24 clips\n",
      "\n",
      "Now that we know the number of clips sold in both months, we can add them up to find the total sales:\n",
      "\n",
      "Total sales = Number of clips sold in April + Number of clips sold in May = 48 + 24 = 72 clips\n",
      "\n",
      "Therefore, Natalia sold a total of 72 clips altogether in April and May.\n",
      "========================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctness_reward_func(prompts=[dataset[0]['prompt']],\n",
    "                        completions=completions,\n",
    "                        answer=dataset[0]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"  \n",
    "    检查模型输出是否为有效整数，并根据结果给予奖励。\n",
    "\n",
    "    输入：\n",
    "    completions: 由模型回答组成的列表。\n",
    "\n",
    "    输出：\n",
    "    奖励分数列表。\n",
    "    \"\"\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `responses = [completion[0]['content'] for completion in completions]`: 提取模型回答的内容\n",
    "- `extracted_responses = [extract_xml_answer(r) for r in responses]`: 从模型回答中提取答案\n",
    "- `return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]`: 检查回答的答案是否为整数形式。如果是则给予奖励 0.5；否则不给予奖励。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_reward_func(completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 强格式检验\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"  \n",
    "    检查模型输出是否符合严格的格式要求。\n",
    "\n",
    "    输入：\n",
    "    completions: 由模型回答组成的列表。\n",
    "\n",
    "    输出：\n",
    "    奖励分数列表。\n",
    "    \"\"\"\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ` pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"`: 正则表达式，确保输出格式严格符合要求\n",
    "- `responses = [completion[0][\"content\"] for completion in completions]`: 从模型回答中提取问题答案\n",
    "- `matches = [re.match(pattern, r) for r in responses]`: 通过正则表达式检查每个回答是否符合格式\n",
    "- `return [0.5 if match else 0.0 for match in matches]`: 如果格式匹配，则给予奖励 0.5；否则不给予奖励"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strict_format_reward_func(completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 弱格式检验\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"  \n",
    "    检查模型的输出是否符合稍微宽松的格式要求。\n",
    "\n",
    "    输入：\n",
    "    completions: 由模型回答组成的列表。\n",
    "\n",
    "    输出：\n",
    "    奖励分数列表。\n",
    "    \"\"\"\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"`: 定义了一个稍微宽松的表达式\n",
    "- `responses = [completion[0][\"content\"] for completion in completions]`: 从模型回答中提取问题答案\n",
    "- `matches = [re.match(pattern, r) for r in responses]`: 检查每个回答是否符合此格式\n",
    "- `return [0.5 if match else 0.0 for match in matches]`: 如果格式匹配则给予奖励 0.5；否则不给予奖励"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_format_reward_func(completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标签计数检验\n",
    "def count_xml(text) -> float:\n",
    "    \"\"\"  \n",
    "    计算文本中标签的出现次数，本根据他们的位置和频率分配奖励。\n",
    "\n",
    "    输入：\n",
    "    text: 计数文本。\n",
    "\n",
    "    输出：\n",
    "    count: 标签总分。\n",
    "    \"\"\"\n",
    "    count = 0.0\n",
    "    if text.count(\"<reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `count = 0.0`: 初始化计数器\n",
    "- `if text.count(\"<reasoning>\\n\") == 1:`: 检查是否有且仅有一个`<reasoning>\\n`标签\n",
    "- `count += 0.125`: 条件满足则奖励 0.125\n",
    "- `if text.count(\"\\n</reasoning>\\n\") == 1:`: 检查是否有且仅有一个`\\n<reasoning>`标签\n",
    "- `count += 0.125`: 条件满足则奖励 0.125\n",
    "- `if text.count(\"\\n<answer>\\n\") == 1:`: 检查是否有且仅有一个`\\n<answer>\\n`标签\n",
    "- `count += 0.125`: 条件满足则奖励 0.125\n",
    "- `count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001`: 如果在标签`\\n<answer>\\n`之后存在多余的文本，按照文本长度扣除一些奖励\n",
    "- `if text.count(\"\\n</answer>\") == 1:`: 检查是否有且仅有一个`\\n</answer>`标签\n",
    "- `count += 0.125`: 条件满足则奖励 0.125\n",
    "- `count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001`: 如果存在多余文本则根据其长度扣除一些奖励"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_xml(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结构符合度检验\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"  \n",
    "    计算每个模型输出的 XML 结构符合度，并给予奖励。\n",
    "\n",
    "    输入：\n",
    "    completions: 由模型回答组成的列表。\n",
    "\n",
    "    输出：\n",
    "    奖励分数列表。\n",
    "    \"\"\"\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [count_xml(c) for c in contents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `contents = [completion[0][\"content\"] for completion in completions]`: 从模型回答中提取答案\n",
    "- `return [count_xml(c) for c in contents]`: 把`count_xml()`输出的结果作为奖励分数并返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmlcount_reward_func(completions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 GRPO 训练流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 训练参数设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从指定位置读取模型\n",
    "model_name = \"./Qwen2.5-0.5B-Instruct\"\n",
    "# 指定模型训练中间值的输出地址\n",
    "output_dir=\"outputs/Qwen-0.5B-GRPO\"\n",
    "# 指定项目运行的名字\n",
    "run_name=\"Qwen-0.5B-GRPO-gsm8k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    # 核心训练参数\n",
    "    output_dir=output_dir,\n",
    "    run_name=run_name,\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    # 训练过程控制\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_generations=16,\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=200,\n",
    "    num_train_epochs=1,\n",
    "    # 系统与资源管理\n",
    "    save_steps=100,\n",
    "    max_grad_norm=0.1,\n",
    "    log_on_each_node=False,\n",
    "    use_vllm=False,\n",
    "    vllm_gpu_memory_utilization=.3,\n",
    "    vllm_device=\"cuda:0\",\n",
    "    report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面这些是训练模型需要指定的一些参数值，这里不做细致介绍，仅简单指明其作用：\n",
    "- 核心训练参数：\n",
    "  - `output_dir: 指定训练结果（模型 checkpoint、日志等）的保存目录\n",
    "  - `run_name`: 为当前训练实验命名，用于日志记录和可视化追踪\n",
    "  - `learning_rate`: 设置初始学习率大小，控制参数更新步长\n",
    "  - `adam_beta1`: Adam 优化器的一阶矩估计衰减率（动量参数）\n",
    "  - `adam_beta2`: Adam 优化器的二阶矩估计衰减率（自适应学习率参数）\n",
    "  - `weight_decay`: L2 正则化系数，防止过拟合\n",
    "  - `warmup_ratio=0.1`: 学习率预热比例（前 10% 的步骤线性增加学习率）\n",
    "  - `lr_scheduler_type`: 使用余弦退火学习率调度策略\n",
    "- 训练过程控制：\n",
    "  - `logging_steps`: 每一个训练步记录一次日志\n",
    "  - `bf16`: 启用 bfloat16 混合精度训练\n",
    "  - `pre_device_train_batch_size`: 每个 GPU 的训练批次大小\n",
    "  - `gradient_accumulation_steps`: 梯度累积步数\n",
    "  - `num_generations`: 每个训练步骤生成 16 个候选响应用于策略优化\n",
    "  - `max_prompt_length`: 输入提示的最大 token 长度限制\n",
    "  - `max_completion_length`: 生成相应的最大 token 长度限制\n",
    "  - `num_train_epochs`: 训练总轮次（完整遍历数据集的次数）\n",
    "- 系统与资源管理：\n",
    "  - `save_steps=100`: 每 100 个训练步保存一次模型检查点\n",
    "  - `max_grad_norm`: 梯度裁剪阈值\n",
    "  - `log_on_each_node=False`: 在分布式训练中仅主节点记录日志\n",
    "  - `use_vllm=False`: 禁用 vLLM 推理加速框架\n",
    "  - `vllm_gpu_memory_utilization=.3`: 当启用 vLLM 时，GPU 显存利用率上限为 30%\n",
    "  - `vllm_device=\"cuda:0\"`: 指定 vLLM 使用 GPU 设备\n",
    "  - `report_to=\"wandb\"`: 将训练指标同步到 wandb 可视化平台"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 加载模型和分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=None\n",
    ").to(\"cuda\")\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 设置网页端监控"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wandb (Weights & Biases) 是一个在线的用于记录模型训练中间过程的网页平台，安装该工具后，模型训练的中间值、日志等信息都会实现显示在网页界面中。\n",
    "\n",
    "如果不用 wandb，那么可能我们 ipynb 的后端进程是开着的，但 ipynb 页面本身已经关掉了，于是就无法看到训练的实时进度和参数变化；而如果连接了 wandb，所有中间信息就都不会错过了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: wandb in /root/autodl-tmp/home/deepseek_tutorial/lib/python3.10/site-packages (0.19.8)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /root/autodl-tmp/home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (2.22.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /root/autodl-tmp/home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /root/autodl-tmp/home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /root/autodl-tmp/home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /root/autodl-tmp/home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: pyyaml in /root/autodl-tmp/home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: setproctitle in /root/autodl-tmp/home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (1.3.5)\n",
      "Requirement already satisfied: platformdirs in /root/autodl-tmp/home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /root/autodl-tmp/home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /root/autodl-tmp/home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (5.29.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /root/autodl-tmp/home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /root/autodl-tmp/home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (2.10.6)\n",
      "Requirement already satisfied: setuptools in /root/autodl-tmp/home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (63.2.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /root/autodl-tmp/home/deepseek_tutorial/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /root/autodl-tmp/home/deepseek_tutorial/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /root/autodl-tmp/home/deepseek_tutorial/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /root/autodl-tmp/home/deepseek_tutorial/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/autodl-tmp/home/deepseek_tutorial/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/autodl-tmp/home/deepseek_tutorial/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/autodl-tmp/home/deepseek_tutorial/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/autodl-tmp/home/deepseek_tutorial/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /root/autodl-tmp/home/deepseek_tutorial/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 要使用 wandb，首先需要在当前环境中安装对应的包\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注册 wandb 请访问 [官网](https://wandb.ai/site/)：\n",
    "\n",
    "![image-20250308160049131](https://asdfdasgasd.oss-cn-chengdu.aliyuncs.com/typora_pictures/20250308160049197.png)\n",
    "\n",
    "![image-20250308160641373](https://asdfdasgasd.oss-cn-chengdu.aliyuncs.com/typora_pictures/20250308160641430.png)\n",
    "\n",
    "![image-20250308160830566](https://asdfdasgasd.oss-cn-chengdu.aliyuncs.com/typora_pictures/20250308160830612.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"your-wandb-key\")       # 输入刚才复制的 api key\n",
    "wandb.init(project=\"deepseek_tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行上面这个代码块后得到的输出中有一个网页链接，点进去就是 wandb 的监控界面了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 开始模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型这一步对显卡有要求，建议从 autodl 上租赁机器来训练。\n",
    "\n",
    "autodl 租机器教程参考仓库中的 autodl_tutorial.md 文档。\n",
    "\n",
    "我租赁的机器是显存 24 GB 的 4090D，价格为 1.98元/时，总的训练时间约为 5-7 h，所以总开销不到 20 元，你就可以自己用 GRPO 让一个 Qwen2.5-0.5B-Instruct 模型学会推理啦。机器具体规格如下：\n",
    "\n",
    "![image-20250308161510563](https://asdfdasgasd.oss-cn-chengdu.aliyuncs.com/typora_pictures/20250308161510617.png)\n",
    "\n",
    "**注意事项：**\n",
    "\n",
    "（下面这些操作如“将数据迁移至数据盘”、“扩容数据盘”都会在 autodl_tutorial.md 文档中介绍到，建议先学习该文档）\n",
    "\n",
    "- 建议租赁 4090D 或 4090，显卡必须 20GB 以上，否则可能会爆\n",
    "- 模型训练过程中会保存很多中间值 (checkpoint)，这些会占用很多空间，根据我的实验来看大概需要 60 GB 存储空间，因此需要注意如下两点：\n",
    "  - 将模型训练文件夹放在数据盘中而非系统盘，因为系统盘只有 30 GB 空间且不可扩容，而数据盘有 50GB 且可以扩容\n",
    "  - 初始的数据盘只有 50 GB，不到 60 GB，大概率不够用，所以需要付费来扩容数据盘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOTrainer\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        xmlcount_reward_func,\n",
    "        soft_format_reward_func,\n",
    "        strict_format_reward_func,\n",
    "        int_reward_func,\n",
    "        correctness_reward_func],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 每隔指定步将模型权重保存到指定文件夹\n",
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完整输出太长了，所以我把输出截成四段展示在下面，最终你得到的应该也是类似的输出效果。\n",
    "\n",
    "训练时间可能不等，不是所有人都能幸运地有 520 这样的训练时间哦～\n",
    "\n",
    "<img src=\"https://asdfdasgasd.oss-cn-chengdu.aliyuncs.com/typora_pictures/20250308114050359.png\" alt=\"image1\" />\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td style=\"vertical-align: top;\">\n",
    "      <img src=\"https://asdfdasgasd.oss-cn-chengdu.aliyuncs.com/typora_pictures/20250308115124913.png\" alt=\"image1\" style=\"zoom:33%;\">\n",
    "    </td>\n",
    "    <td style=\"vertical-align: top;\">\n",
    "      <img src=\"https://asdfdasgasd.oss-cn-chengdu.aliyuncs.com/typora_pictures/20250308114245613.png\" alt=\"image2\" style=\"zoom:33%;\">\n",
    "    </td>\n",
    "    <td style=\"vertical-align: top;\">\n",
    "      <img src=\"https://asdfdasgasd.oss-cn-chengdu.aliyuncs.com/typora_pictures/20250308114420673.png\" alt=\"image3\" style=\"zoom:33%;\">\n",
    "    </td>\n",
    "    <td style=\"vertical-align: top;\">\n",
    "      <img src=\"https://asdfdasgasd.oss-cn-chengdu.aliyuncs.com/typora_pictures/20250308114521214.png\" alt=\"image4\" style=\"zoom:33%;\">\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "详细的 log 信息见`train_log.txt`文件。​    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 运行模型并查看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定模型所在目录\n",
    "grpo_model_name = \"./outputs/Qwen-0.5B-GRPO\"\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    grpo_model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 构建对话消息\n",
    "prompt = \"Joy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<reasoning>\n",
      "First, we need to find out how many pages Joy reads per minute: 8 pages / 20 minutes = 0.4 pages per minute. Then we calculate how long it takes her to read 120 pages at this rate: 120 pages / (0.4 pages/minute) = 300 minutes. Since there are 60 minutes in an hour, we convert the time from minutes to hours: 300 minutes / 60 minutes/hour = 5 hours.\n",
      "</reasoning>\n",
      "<answer>\n",
      "5\n",
      "</answer>\n",
      "<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# 串联对话消息以构建对话历史\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "# 转化为模型输入\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 获取模型输出并转化为 token\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够看出，对比此前原始模型的状态，现在的模型已经能够顺利产生思考过程，且严格按照规定格式回答问题。\n",
    "\n",
    "恭喜你已经成功掌握为 DeepSeek 赋予推理能力的关键技术 GRPO 啦～"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
