{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeek GRPO 训练流程\n",
    "在本文件中，我们将手动实现一个 GRPO 并使用它为 Qwen2.5 模型添加类似于 DeepSeek 的推理能力，旨在了解 GRPO 的技术原理并掌握其基本实现方法。\n",
    "\n",
    "## 1 初始化模型并构建聊天模板\n",
    "### 1.1 安装所需依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting modelscope\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/61/e8/4e40eb57d77d64656067528c59d130120d3e3ea343fba0aaf50776111bb1/modelscope-1.23.2-py3-none-any.whl (5.9 MB)\n",
      "Collecting requests>=2.25\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.64.0\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting urllib3>=1.26\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/c8/19/4ec628951a74043532ca2cf5d97b7b14863931476d117c471e8e2b1eb39f/urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/38/fc/bce832fd4fd99766c04d1ee0eead6b0ec6486fb100ae5e74c1d91292b982/certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/93/62/5e89cdfe04584cb7f4d36003ffa2936681b03ecc0754f8e969c2becb7e24/charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (146 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.1/146.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3, tqdm, idna, charset-normalizer, certifi, requests, modelscope\n",
      "Successfully installed certifi-2025.1.31 charset-normalizer-3.4.1 idna-3.10 modelscope-1.23.2 requests-2.32.3 tqdm-4.67.1 urllib3-2.3.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting torch\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/37/81/aa9ab58ec10264c1abe62c8b73f5086c3c558885d6beecebf699f0dbeaeb/torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.2.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/01/65/3ffa90e158a2c82f0716eee8d26a725d241549b7d7aaf7e4f44ac03ebd89/triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sympy==1.13.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/b2/fe/81695a1aa331a842b582453b605175f419fe8540355886031328089d840a/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/df/99/12cd266d6233f47d00daf3a72739872bdc10267d0383508b0b9c84a18bb6/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/ae/71/1c91302526c45ab494c23f61c7a84aa568b8c1f9d196efa5993957faf906/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/db/f7/97a9ea26ed4bbbfc2d470994b8b4f338ef663be97b8f677519ac195e113d/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/89/ec/00d68c4ddfedfe64159999e5f8a98fb8442729a63e2077eb9dcd89623d27/filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/27/94/3266821f65b92b3138631e9c8e7fe1fb513804ac934485a8d05776e1dd43/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/9f/fd/713452cd72343f682b1c7b9321e23829f00b842ceaedcda96e742ea0b0b3/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.10.0 in /home/deepseek_tutorial/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/87/20/199b8713428322a2f22b722c62b8cc278cc53dffa9705d744484b5035ee9/nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/8a/6d/44ad094874c6f1b9c654f8ed939590bdc408349f137f9b98a3a23ccec411/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/e2/94/758680531a00d06e471ef649e4ec2ed6bf185356a7f9fbfbb7368a40bd49/fsspec-2025.2.0-py3-none-any.whl (184 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 kB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/ea/27/1795d86fe88ef397885f2e580ac37628ed058a92ed2c39dc8eac3adf0619/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jinja2\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/62/a1/3d680cbfd5f4b8f15abc1d571870c5fc3e594bb582bc3b64ea099db13e56/jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.2\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/78/a8/bcbb63b53a4b1234feeafb65544ee55495e1bb37ec31b999b963cbccfd1d/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.4.127\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/ff/ff/847841bacfbefc97a00036e0fce5a0f086b640756dc38caea5e1bb002655/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/3a/e1/5b9089a4b2a4790dfdea8b3a006052cfecff58139d5a4e34cb1a51df8d6f/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.4.127\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/2c/14/91ae57cd4db3f9ef7aa99f4019cfa8d54cb4caa7e00975df6467e9725a9f/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/b9/54/dd730b32ea14ea797530a4479b2ed46a6fb250f682a9cfb997e968bf0261/networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/67/42/f4f60238e8194a3106d06a058d494b18e006c10bb2b915655bd9f6ea4cb1/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting mpmath<1.4,>=1.1.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/43/e3/7d92a15f894aa0c9c4b49b8ee9ac9850d6e63b03c9c32c0367a13ae62209/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m161.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting MarkupSafe>=2.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/22/35/137da042dfb4720b638d2937c38a9c2df83fe32d20e8c8f3185dbfef05f7/MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Installing collected packages: triton, nvidia-cusparselt-cu12, mpmath, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.17.0 fsspec-2025.2.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0 triton-3.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting transformers\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/20/37/1f29af63e9c30156a3ed6ebc2754077016577c094f31de7b2631e5d379eb/transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/13/5f/088ff08dc41808fcd99d9972b9bcfa7e3a35e30e8b0a3155b57938f1611c/huggingface_hub-0.29.2-py3-none-any.whl (468 kB)\n",
      "Requirement already satisfied: filelock in /home/deepseek_tutorial/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/deepseek_tutorial/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/6b/4e/1523cb902fd98355e2e9ea5e5eb237cbc5f3ad5f3075fa65087aa0ecb669/PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/deepseek_tutorial/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/22/06/69d7ce374747edaf1695a4f61b83570d91cc8bbfc51ccfecf76f56ab4aac/tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Collecting numpy>=1.17\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/e9/88/3870cfa9bef4dffb3a326507f430e6007eeac258ebeef6b76fc542aef66d/numpy-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.1\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/a6/f8/dae3421624fcc87a89d42e1898a798bc7ff72c61f38973a65d60df8f124c/safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/f2/98/26d3830875b53071f1f0ae6d547f1d98e964dd29ad35cbf94439120bb67a/regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/deepseek_tutorial/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/deepseek_tutorial/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/deepseek_tutorial/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/deepseek_tutorial/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/deepseek_tutorial/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/deepseek_tutorial/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/deepseek_tutorial/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Installing collected packages: safetensors, regex, pyyaml, numpy, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.29.2 numpy-2.2.3 pyyaml-6.0.2 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.0 transformers-4.49.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting trl\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/7b/29/25378447c48359843de0e4ce1995d367210601c3b437ddf1c779b6393d74/trl-0.15.2-py3-none-any.whl (318 kB)\n",
      "Requirement already satisfied: transformers>=4.46.0 in /home/deepseek_tutorial/lib/python3.10/site-packages (from trl) (4.49.0)\n",
      "Collecting datasets>=2.21.0\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/4c/37/22ef7675bef4ffe9577b937ddca2e22791534cbbe11c30714972a91532dc/datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "Collecting rich\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/19/71/39c7c0d87f8d4e6c020a393182060eaefeeae6c01dab6a84ec346f2567df/rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Collecting accelerate>=0.34.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/0a/f6/791b9d7eb371a2f385da3b7f1769ced72ead7bf09744637ea2985c83d7ee/accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<3.0.0,>=1.17 in /home/deepseek_tutorial/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (2.2.3)\n",
      "Requirement already satisfied: pyyaml in /home/deepseek_tutorial/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (6.0.2)\n",
      "Requirement already satisfied: psutil in /home/deepseek_tutorial/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (7.0.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/deepseek_tutorial/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (0.5.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/deepseek_tutorial/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (24.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/deepseek_tutorial/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/deepseek_tutorial/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (0.29.2)\n",
      "Collecting fsspec[http]<=2024.12.0,>=2023.1.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/de/86/5486b0188d08aa643e127774a99bac51ffa6cf343e3deb0583956dca5b22/fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.9,>=0.3.0\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/c9/7a/cef76fd8438a42f96db64ddaa85280485a9c395e7df3db8158cfec1eee34/dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/deepseek_tutorial/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (4.67.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/deepseek_tutorial/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (2.32.3)\n",
      "Collecting aiohttp\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/fa/9e/d0bbdc82236c3fe43b28b3338a13ef9b697b0f7a875b33b950b975cab1f6/aiohttp-3.11.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "Collecting xxhash\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/f2/07/d9a3059f702dec5b3b703737afb6dda32f304f6e9da181a229dafd052c29/xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Collecting multiprocess<0.70.17\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/bc/f7/7ec7fddc92e50714ea3745631f79bd9c96424cb2702632521028e57d3a36/multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Collecting pandas\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/44/50/7db2cd5e6373ae796f0ddad3675268c8d59fb6076e66f0c339d61cea886b/pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/ef/c9/68ab123ee1528699c4d5055f645ecd1dd68ff93e4699527249d02f55afeb/pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "Requirement already satisfied: filelock in /home/deepseek_tutorial/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.17.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/deepseek_tutorial/lib/python3.10/site-packages (from transformers>=4.46.0->trl) (0.21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/deepseek_tutorial/lib/python3.10/site-packages (from transformers>=4.46.0->trl) (2024.11.6)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /home/deepseek_tutorial/lib/python3.10/site-packages (from rich->trl) (4.12.2)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/deepseek_tutorial/lib/python3.10/site-packages (from rich->trl) (2.19.1)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/ee/59/928322800306f6529d1852323014ee9008551e9bb027cc38d276cbc0b0e7/frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/ec/6a/bc7e17a3e87a2985d3e8f4da4cd0f481060eb78fb08596c42be62c90a4d9/aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/a9/42/b1753949b327b36f210899f2dd0a0947c0c74e42a32de3f8eb5c7d93edca/yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/fe/ba/e2081de779ca30d473f21f5b30e0e737c438205440784c7dfc81efc2b029/async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/a6/64/2dd6c4c681688c0165dea3975a6a4eab4944ea30f35000f8b8af1df3148c/multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/f4/eb/41447de61eb5454891658d0fb9b1d7d35d49a4a5dd2e0c86f2c332e8b7e1/propcache-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/1b/9a/e4886864ce06e1579bd428208127fbdc0d62049c751e4e9e3b509c0059dc/aiohappyeyeballs-2.5.0-py3-none-any.whl (15 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/fc/30/d4986a882011f9df997a55e6becd864812ccfcd821d64aac8570ee39f719/attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.2/63.2 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mdurl~=0.1\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/deepseek_tutorial/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/deepseek_tutorial/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/deepseek_tutorial/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2025.1.31)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/deepseek_tutorial/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/deepseek_tutorial/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/deepseek_tutorial/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/deepseek_tutorial/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/deepseek_tutorial/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/deepseek_tutorial/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
      "Requirement already satisfied: jinja2 in /home/deepseek_tutorial/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/deepseek_tutorial/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
      "Requirement already satisfied: networkx in /home/deepseek_tutorial/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.4.2)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/deepseek_tutorial/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/deepseek_tutorial/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/deepseek_tutorial/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.6.1.9)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/deepseek_tutorial/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/deepseek_tutorial/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/deepseek_tutorial/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/deepseek_tutorial/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (0.6.2)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/deepseek_tutorial/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/deepseek_tutorial/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (9.1.0.70)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/deepseek_tutorial/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/0f/dd/84f10e23edd882c6f968c21c2434fe67bd4a528967067515feca9e611e5e/tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/deepseek_tutorial/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/eb/38/ac33370d784287baa1c3d538978b5e2ea064d4c1b93ffbd12826c190dd10/pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/deepseek_tutorial/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/deepseek_tutorial/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.2)\n",
      "Installing collected packages: pytz, xxhash, tzdata, pyarrow, propcache, multidict, mdurl, fsspec, frozenlist, dill, attrs, async-timeout, aiohappyeyeballs, yarl, pandas, multiprocess, markdown-it-py, aiosignal, rich, aiohttp, accelerate, datasets, trl\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.2.0\n",
      "    Uninstalling fsspec-2025.2.0:\n",
      "      Successfully uninstalled fsspec-2025.2.0\n",
      "Successfully installed accelerate-1.4.0 aiohappyeyeballs-2.5.0 aiohttp-3.11.13 aiosignal-1.3.2 async-timeout-5.0.1 attrs-25.1.0 datasets-3.3.2 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.12.0 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.1.0 multiprocess-0.70.16 pandas-2.2.3 propcache-0.3.0 pyarrow-19.0.1 pytz-2025.1 rich-13.9.4 trl-0.15.2 tzdata-2025.1 xxhash-3.5.0 yarl-1.18.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install modelscope\n",
    "%pip install torch\n",
    "%pip install transformers\n",
    "%pip install trl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 下载模型\n",
    "为了尽可能减少训练成本，我们使用参数量为 0.5B 的 Qwen2.5 模型作为基座模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 在本地创建文件夹用于保存模型参数\n",
    "os.makedirs(\"./Qwen2.5-0.5B-Instruct\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# 从 modelscope 库下载 Qwen2.5 模型到指定文件夹\n",
    "!modelscope download --model Qwen/Qwen2.5-0.5B-Instruct --local_dir ./Qwen2.5-0.5B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 加载模型与分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deepseek_tutorial/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 指定模型所在目录\n",
    "model_name = \"./Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AutoModelForCausalLM.from_pretrained()`:\n",
    "\n",
    "**核心功能：**\n",
    "\n",
    "- 从指定路径加载模型\n",
    "- 支持动态选择计算精度并分配设备\n",
    "\n",
    "**参数解析：**\n",
    "\n",
    "- `pretrained_model_name_or_path`: 基座模型路径。\n",
    "- `torch_dtype`: 选择张量数据类型。\"auto\"表示根据硬件能力自动选择张量数据类型(float32, float16, bfloat16)，在保持精度的同时最大化计算效率。\n",
    "- `device_map`: 选择设备分配策略。\"auto\"表示根据硬件能力自动分配 CPU/GPU 资源。\n",
    "\n",
    "`AutoTokenizer.from_pretrained()`:\n",
    "\n",
    "**核心功能：**\n",
    "\n",
    "- 从指定路径加载文件的 tokenizer(分词器)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 创建对话消息\n",
    "平时我们在使用大语言模型应用进行对话时，在聊天框中输入的内容就称为 prompt。输入的 prompt 在被传输到后台服务器时，会被组织成`{\"role\": \"user\", \"content\": <你输入的 prompt>}`这样的形式（注：这种数据类型叫做`json`，大家之后在数据处理任务中会经常使用到它），用于表示这是用户发送的消息。\n",
    "\n",
    "通常而言，在聊天这一应用场景中，每条消息均以 json 格式定义，包含两个关键字段：\n",
    "\n",
    "- `role`: 标识消息来源，支持三种标准角色：\n",
    "  - `user`: 用户发送的消息（输入的 prompt）\n",
    "  - `assistant`: 模型生成的回复\n",
    "  - `system`: 系统级指令（用于预设模型行为，通常仅出现在首条消息）\n",
    "- `content`: 消息的文本内容\n",
    "\n",
    "因此，用户和系统之间的消息你来我往，便能够构成如下所示的消息历史：\n",
    "\n",
    "```json\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"......\"},\n",
    "  {\"role\": \"user\", \"content\": \"......\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"......\"},\n",
    "  {\"role\": \"user\", \"content\": \"......\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"......\"},\n",
    "  ......\n",
    "]\n",
    "```\n",
    "\n",
    "每一次回答时，消息历史中的所有内容都会被作为模型输入，因此你会发现模型总能够“记住”你们之间说过的话（实际上，模型输入有一定的长度限制，即“上下文窗口长度”，通常按照 token 数量来计算，比如 4K/16K tokens，超出长度的部分会被自动截断）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 原始消息结构 =====\n",
      " [{'role': 'user', 'content': 'Joy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?'}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Joy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "print(\"\\n===== 原始消息结构 =====\\n\", messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面说到，模型和用户的对话历史会被组织成 json 列表的形式传输给模型，接下来便需要模型对上述消息作出回答。\n",
    "\n",
    "我们知道，语言模型所做的任务本质上就是“文字接龙”，而聊天是一项比较特殊的“文字接龙”，因为它加入了身份的切换，从而让输出看起来像是一场对话。因此同其他任务一样，在聊天这项任务中，模型在作出回答时，首先也需要先做 tokenize 操作，也就是将大段的文字切分成一个又一个模型认识的小字段，也就是 token。\n",
    "\n",
    "为了将`messages`格式化为模型输入，需要调用`tokenizer.apply_chat_template()`方法。并为该方法设定了两个参数值：`tokenize=False`和`add_generation_prompt=True`。\n",
    "\n",
    "怎么理解这两个参数呢？\n",
    "\n",
    "首先，`add_chat_template()`会将传入的 json 列表`messages`拼接为一段文本。接着，如果`tokenize=True`，则这一段文本会被直接被切分成 tokens，最终输出由各个 token 的 id 组成的数字序列；如果`tokenize=False`，则会保留文本原本的样式。具体区别可以对比下面代码块中的`text`和`text_1`。\n",
    "\n",
    "然后是`add_generation_prompt`这个参数。简单来说，如果`add_generation_prompt=True`，那么每次在`<|im_start|>user......<|im_end|>`后，都会自动接一个`<|im_start|>assistant`，用来告诉模型：“嘿，用户已经说完了，现在该你回答了”；如果`add_generation_prompt=False`，则不会自动接这个提示词。不过一般来讲，在实操中我们都会选择`add_generation_prompt=True`，就像上面所说的，我们是在用一个“文字接龙”的机器模拟“聊天”，因此如果不刻意地给模型一些提示，它可能会接着补充用户的内容，而不是“发表”自己的意见。具体区别可以对比下面代码块中的`text`和`text_2`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== text =====\n",
      " <|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Joy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "===== text_1 =====\n",
      " [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 79771, 646, 1349, 220, 23, 6816, 315, 264, 2311, 304, 220, 17, 15, 4420, 13, 2585, 1657, 4115, 686, 432, 1896, 1059, 311, 1349, 220, 16, 17, 15, 6816, 30, 151645, 198, 151644, 77091, 198]\n",
      "\n",
      "===== text_2 =====\n",
      " <|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Joy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "text_1 = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "text_2 = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False\n",
    ")\n",
    "\n",
    "print(\"\\n===== text =====\\n\", text)\n",
    "print(\"\\n===== text_1 =====\\n\", text_1)\n",
    "print(\"\\n===== text_2 =====\\n\", text_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 将文本格式化为模型输入\n",
    "截至目前我们已经获得了经过`apply_chat_template`格式化后的`text`，接下来需要将文本格式化为模型能够理解的输入，这一过程需要使用的是`tokenizer`，最终返回的则是一个列表，列表由每个 token 在词汇表中对应的 id 组成，也就是代码块输出的`input_ids`。\n",
    "\n",
    "此外，输出中还包含`attention_mask`和`device`，其中`attention_mask`用来标识哪些位置是有效 token（1=有效，0=填充位）；`device`将张量移动到模型所在设备，因为模型参数和张量必须在同一设备上才能计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "             13, 151645,    198, 151644,    872,    198,  79771,    646,   1349,\n",
      "            220,     23,   6816,    315,    264,   2311,    304,    220,     17,\n",
      "             15,   4420,     13,   2585,   1657,   4115,    686,    432,   1896,\n",
      "           1059,    311,   1349,    220,     16,     17,     15,   6816,     30,\n",
      "         151645,    198, 151644,  77091,    198]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer(\n",
    "    [text],                # 输入文本（列表形式，支持批量输入）\n",
    "    return_tensors=\"pt\"    # 返回 PyTorch 张量\n",
    "    ).to(model.device)     # 将张量迁移到模型所在设备\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 获取模型输出并转化为文本\n",
    "接下来我们将输入喂给模型并获取模型输出，第一步先是获取模型新生成的 token 的 id 序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([  1249,   8253,   1246,   1293,    432,    686,   1896,  27138,    311,\n",
      "          1349,    220,     16,     17,     15,   6816,     11,    582,   1156,\n",
      "          1184,    311,   1477,    700,   1059,   5290,   4379,    304,   6816,\n",
      "           817,   9383,     13,   1205,   1414,    429,   1340,  15804,    220,\n",
      "            23,   6816,    304,    220,     17,     15,   4420,     13,  15277,\n",
      "            11,   1059,   5290,   4379,    374,   1447,     59,   9640,     59,\n",
      "          1318,     90,  31899,   4379,     92,    284,   1124,  37018,     90,\n",
      "            23,   1124,   1318,     90,   6816,   3417,     90,     17,     15,\n",
      "          1124,   1318,     90,   4420,   3417,    284,   1124,  37018,     90,\n",
      "            17,   1124,   1318,     90,   6816,   3417,     90,     20,   1124,\n",
      "          1318,     90,   4420,  11248,     59,   2533,   1986,   3363,    429,\n",
      "         27138,  15804,    518,    264,   4379,    315,   1124,  11520,  37018,\n",
      "            90,     17,  15170,     20,  11035,      8,   6816,    817,   9383,\n",
      "            13,   2014,   1477,    700,   1246,   1293,    432,   4990,   1059,\n",
      "           311,   1349,    220,     16,     17,     15,   6816,     11,    582,\n",
      "           990,    279,  14806,   1447,     59,   9640,     59,   1318,     90,\n",
      "          1462,     92,    284,   1124,  37018,  35702,   1318,     90,   7595,\n",
      "          6816,   3417,  35702,   1318,     90,  31899,   4379,  11248,     59,\n",
      "          2533,   3136,   3696,  10607,    279,   3881,   2750,   1447,     59,\n",
      "          9640,     59,   1318,     90,   1462,     92,    284,   1124,  37018,\n",
      "            90,     16,     17,     15,   1124,   1318,     90,   6816,   3417,\n",
      "         35702,  37018,     90,     17,  15170,     20,     92,   1124,   1318,\n",
      "            90,   6816,     14,  42557,   3417,    284,    220,     16,     17,\n",
      "            15,   1124,  15136,   1124,  37018,     90,     20,  15170,     17,\n",
      "            92,    284,    220,     21,     15,   1124,  15136,    220,     20,\n",
      "           284,    220,     18,     15,     15,   1124,   1318,     90,   4420,\n",
      "           532,     59,   2533,  12549,   1052,    525,    220,     21,     15,\n",
      "          4420,    304,    458,   6460,     11,    582,   5508,    279,    882,\n",
      "           504,   4420,    311,   4115,   1447,     59,   9640,     18,     15,\n",
      "            15,   1124,   1318,     90,   4420,     92,    284,   1124,  37018,\n",
      "            90,     18,     15,     15,  15170,     21,     15,     92,   1124,\n",
      "          1318,     90,   4115,     92,    284,    220,     20,   1124,   1318,\n",
      "            90,   4115,    532,     59,   2533,  54815,     11,    432,    686,\n",
      "          1896,  27138,   1124,  11520,  79075,     90,     20,  11035,      8,\n",
      "          4115,    311,   1349,    220,     16,     17,     15,   6816,     13,\n",
      "        151645], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512    # 限制模型生成新的 token 数不超过 512\n",
    ")\n",
    "# 遍历每个输入样本的 input_ids 和对应的生成结果 output_ids\n",
    "# 对每个样本，计算输入长度 len(input_ids)，截取生成结果中从该位置到末尾的部分\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):]\n",
    "    for input_ids, output_ids\n",
    "    in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "print(generated_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后将 id 转化为对应的 token 文本，最终组合成一个字符串，即模型的回答。\n",
    "\n",
    "`skip_special_tokens=True`表明跳过特殊标记如`<|im_start|>`,`<|im_end|>`等，这是因为特殊标记虽然对模型有意义，但对用户无意义，所以通常需要跳过。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine how long it will take Joy to read 120 pages, we first need to find out her reading rate in pages per minute. We know that she reads 8 pages in 20 minutes. Therefore, her reading rate is:\n",
      "\n",
      "\\[\n",
      "\\text{Reading rate} = \\frac{8 \\text{ pages}}{20 \\text{ minutes}} = \\frac{2 \\text{ pages}}{5 \\text{ minutes}}\n",
      "\\]\n",
      "\n",
      "This means that Joy reads at a rate of \\(\\frac{2}{5}\\) pages per minute. To find out how long it takes her to read 120 pages, we use the formula:\n",
      "\n",
      "\\[\n",
      "\\text{Time} = \\frac{\\text{Total pages}}{\\text{Reading rate}}\n",
      "\\]\n",
      "\n",
      "Substituting the known values:\n",
      "\n",
      "\\[\n",
      "\\text{Time} = \\frac{120 \\text{ pages}}{\\frac{2}{5} \\text{ pages/minute}} = 120 \\times \\frac{5}{2} = 60 \\times 5 = 300 \\text{ minutes}\n",
      "\\]\n",
      "\n",
      "Since there are 60 minutes in an hour, we convert the time from minutes to hours:\n",
      "\n",
      "\\[\n",
      "300 \\text{ minutes} = \\frac{300}{60} \\text{ hours} = 5 \\text{ hours}\n",
      "\\]\n",
      "\n",
      "Therefore, it will take Joy \\(\\boxed{5}\\) hours to read 120 pages.\n"
     ]
    }
   ],
   "source": [
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们成功用原生的 Qwen2.5 大模型构建起了一个能够聊天的机器人，但是目前它的回答是糅合在一起的，并不具备像 DeepSeek 那样的推理能力，接下来我们就为它添加推理能力。\n",
    "## 2 尝试用 prompt 注入方式为模型添加推理能力\n",
    "说起为模型添加推理能力，我们首先能够想到的便是通过 prompt 提示模型先给出推理然后再给出答案，那么这种简单朴素的想法是否能够成功呢？让我们一探究竟。\n",
    "\n",
    "这部分需要用到一个名为 gsm8k 的数据集，它是一个广泛用于评估语言模型数学推理能力的基准数据集，包含 8500 个高质量的小学数学应用题，每条数据均包含`question`和`answer`两项内容，且`answer`中不仅有问题的答案，还包含了问题的计算推导过程，因此该数据集常用来测试模型理解和处理数学推理能力。该数据集可以直接从`datasets`库中获取。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autodl 添加学术加速\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== gsm8k 数据信息 =====\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 7473\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer'],\n",
      "        num_rows: 1319\n",
      "    })\n",
      "}) \n",
      "\n",
      "\n",
      "===== gsm8k 训练集数据展示（第一条） =====\n",
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n"
     ]
    }
   ],
   "source": [
    "import datasets as hf_datasets\n",
    "data = hf_datasets.load_dataset('openai/gsm8k', 'main')\n",
    "print(\"\\n===== gsm8k 数据信息 =====\")\n",
    "print(data,\"\\n\")\n",
    "print(\"\\n===== gsm8k 训练集数据展示（第一条） =====\")\n",
    "print(data['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# 关闭学术加速，否则会对正常网络造成影响\n",
    "!unset http_proxy && unset https_proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仔细观察可以看到，`answer`被`####`分成了两部分，前半部分是`reasoning`，后半部分则是直接给出`answer`。现在你可能已经想象到我们最终要赋予给 Qwen2.5 的是一个怎样的推理能力了。\n",
    "\n",
    "对！就是要让它在`reasoning`时给出细致的数学推理过程，在回答时直接给出`answer`。\n",
    "\n",
    "![image-20250305195837974](https://asdfdasgasd.oss-cn-chengdu.aliyuncs.com/typora_pictures/20250305195838053.png)\n",
    "\n",
    "接下来对原始的 gsm8k 数据集进行重构，剔除`answer`中的`reasoning`部分，修改后的结构如下所示：\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"question\": \"对应原数据集中的 question\",\n",
    "  \"answer\": \"仅包含原 answer 中 #### 之后的部分\",\n",
    "  \"prompt\":\n",
    "  \t[\n",
    "  \t\t{\n",
    "  \t\t\t\"role\": \"system\",\n",
    "  \t\t\t\"content\": \"SYSTEM_PROMPT，旨在提示模型按照先给出 reasoning 再给出 answer 的格式回答\",\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"原数据集中的 question\"\n",
    "      }\n",
    "  \t]\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    \"\"\"  \n",
    "    从原始文本中提取 answer(#### 之后的部分)\n",
    "    \"\"\"\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split]\n",
    "    data = data.map(lambda x: {\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': x['question']}\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 重构后的数据集信息 =====\n",
      "Dataset({\n",
      "    features: ['question', 'answer', 'prompt'],\n",
      "    num_rows: 7473\n",
      "})\n",
      "\n",
      "===== 重构后的第一条数据 =====\n",
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': '72', 'prompt': [{'content': '\\nRespond in the following format:\\n<reasoning>\\n...\\n</reasoning>\\n<answer>\\n...\\n</answer>\\n', 'role': 'system'}, {'content': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'role': 'user'}]}\n"
     ]
    }
   ],
   "source": [
    "dataset = get_gsm8k_questions()\n",
    "print(\"\\n===== 重构后的数据集信息 =====\")\n",
    "print(dataset)\n",
    "print(\"\\n===== 重构后的第一条数据 =====\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "细心的你可能已经发现了，重构之后的数据集中，键`prompt`对应的值本身就是一个对话历史，因此我们可以直接把`prompt`取出来作为`messages`喂给大模型，并获取大模型的回答。而且这次，咱们的`prompt`中包含了`SYSTEM_PROMPT`，提示模型先给出推理再给出答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find out how many clips Natalia sold in total in April and May, we need to follow these steps:\n",
      "\n",
      "1. Determine the number of clips sold in May.\n",
      "2. Add the number of clips sold in April and May.\n",
      "\n",
      "First, we know that Natalia sold 48 clips in April. In May, she sold half as many clips as she did in April. So, we calculate the number of clips sold in May as follows:\n",
      "\n",
      "\\[ \\text{Clips sold in May} = \\frac{\\text{Clips sold in April}}{2} = \\frac{48}{2} = 24 \\]\n",
      "\n",
      "Next, we add the number of clips sold in April and May to find the total:\n",
      "\n",
      "\\[ \\text{Total clips sold} = \\text{Clips sold in April} + \\text{Clips sold in May} = 48 + 24 = 72 \\]\n",
      "\n",
      "So, Natalia sold a total of 72 clips in April and May. The final answer is:\n",
      "\n",
      "\\[\n",
      "\\boxed{72}\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "messages = dataset[0]['prompt']\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而结果并不理想，模型并没有诞生思考过程，且结果也没有按照我们限定的格式返回，看来只靠 prompt 这样的“口头要求”来约束模型回答还是远远不够的。\n",
    "\n",
    "专业的事交给专业的“人”，GRPO 这项技术就是专门用来训练模型逻辑推理能力的一种强化学习框架，这项技术由 DeepSeek 团队于 2024 年 4 月在 DeepSeekMath 这篇文章中首次提出，由于本篇教学仅做简单科普，因此关于其数学原理我们暂不讨论，有兴趣的读者可参考 [原论文](https://arxiv.org/abs/2402.03300) 并深入学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 GRPO 奖励函数组复现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先前我们已经将 gsm8k 数据集进行了重组，重组之后每条`prompt`都可以作为对话历史喂给模型从而获得输出。GRPO 做的事情是，对每一条回答都会进行评估，在当前的数学计算场景下，有两项评估指标，分别是“准确性评估”和“格式评估”。如果回答结果准确，则给予模型“准确性奖励”，否则不给予奖励；如果回答格式准确，则给予模型“格式奖励”，否则不给予奖励。\n",
    "\n",
    "下面将一一实现需要的奖励函数，大都是硬性的文本匹配规则，可讲的不多，主要是解释代码的实现方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'To find out how many clips Natalia sold in total in April and May, we need to follow these steps:\\n\\n1. Determine the number of clips sold in May.\\n2. Add the number of clips sold in April and May.\\n\\nFirst, we know that Natalia sold 48 clips in April. In May, she sold half as many clips as she did in April. So, we calculate the number of clips sold in May as follows:\\n\\n\\\\[ \\\\text{Clips sold in May} = \\\\frac{\\\\text{Clips sold in April}}{2} = \\\\frac{48}{2} = 24 \\\\]\\n\\nNext, we add the number of clips sold in April and May to find the total:\\n\\n\\\\[ \\\\text{Total clips sold} = \\\\text{Clips sold in April} + \\\\text{Clips sold in May} = 48 + 24 = 72 \\\\]\\n\\nSo, Natalia sold a total of 72 clips in April and May. The final answer is:\\n\\n\\\\[\\n\\\\boxed{72}\\n\\\\]'}]\n"
     ]
    }
   ],
   "source": [
    "# 封装模型回答\n",
    "completion=  [{'content': response}]\n",
    "completions = [completion]\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml_answer(text: str) -> str:\n",
    "    \"\"\"  \n",
    "    从模型回答的文本中提取数学结果（<answer></answer>之间的内容）。\n",
    "    \"\"\"\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正确性检验\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    \"\"\"  \n",
    "    检查模型输出是否与正确答案相匹配，并根据匹配情况返回奖励分数。\n",
    "\n",
    "    输入：\n",
    "    prompts: 由数据集中各条数据的`prompt`组成的列表，即输入给模型的对话历史。\n",
    "    completions: 由模型回答组成的列表。\n",
    "    answer: 由数据集中各条数据的`answer`组成的列表，即问题的正确答案。\n",
    "\n",
    "    输出：\n",
    "    奖励分数列表。\n",
    "    \"\"\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content']\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    print(\"========================================================\")\n",
    "    print('-'*20, \" Question \", '-'*20, \"\\n\", q)\n",
    "    print('-'*20, \" Answer \", '-'*20, \"\\n\", answer)\n",
    "    print('-'*20, \" Response \", '-'*20, \"\\n\", responses[0])\n",
    "    print('-'*20, \" Extracted \", '-'*20, \"\\n\", extracted_responses[0])\n",
    "    print(\"========================================================\")\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现：\n",
    "\n",
    "- `responses = [completion[0]['content'] for completion in completions]`: 提取每个 competion 中的内容（即模型的输出）\n",
    "- `q = prompts[0][-1]['content']`: 提取输入问题（prompt）的内容\n",
    "- `extracted_responses = [extract_xml_answer(r) for r in responses]`: 用`extract_xml_answer()`方法从模型输出中提取出答案部分\n",
    "- `print(...)`打印问题、正确答案、模型输出和提取的答案\n",
    "- `return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]`: 比较提取出来的每个答案与正确答案。如果两者相同，则返回奖励 2.0；否则返回 0.0。奖励是一个列表，长度与模型的输出数目相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "--------------------  Question  -------------------- \n",
      " Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "--------------------  Answer  -------------------- \n",
      " 72\n",
      "--------------------  Response  -------------------- \n",
      " To find out how many clips Natalia sold in total in April and May, we need to follow these steps:\n",
      "\n",
      "1. Determine the number of clips sold in May.\n",
      "2. Add the number of clips sold in April and May.\n",
      "\n",
      "First, we know that Natalia sold 48 clips in April. In May, she sold half as many clips as she did in April. So, we calculate the number of clips sold in May as follows:\n",
      "\n",
      "\\[ \\text{Clips sold in May} = \\frac{\\text{Clips sold in April}}{2} = \\frac{48}{2} = 24 \\]\n",
      "\n",
      "Next, we add the number of clips sold in April and May to find the total:\n",
      "\n",
      "\\[ \\text{Total clips sold} = \\text{Clips sold in April} + \\text{Clips sold in May} = 48 + 24 = 72 \\]\n",
      "\n",
      "So, Natalia sold a total of 72 clips in April and May. The final answer is:\n",
      "\n",
      "\\[\n",
      "\\boxed{72}\n",
      "\\]\n",
      "--------------------  Extracted  -------------------- \n",
      " To find out how many clips Natalia sold in total in April and May, we need to follow these steps:\n",
      "\n",
      "1. Determine the number of clips sold in May.\n",
      "2. Add the number of clips sold in April and May.\n",
      "\n",
      "First, we know that Natalia sold 48 clips in April. In May, she sold half as many clips as she did in April. So, we calculate the number of clips sold in May as follows:\n",
      "\n",
      "\\[ \\text{Clips sold in May} = \\frac{\\text{Clips sold in April}}{2} = \\frac{48}{2} = 24 \\]\n",
      "\n",
      "Next, we add the number of clips sold in April and May to find the total:\n",
      "\n",
      "\\[ \\text{Total clips sold} = \\text{Clips sold in April} + \\text{Clips sold in May} = 48 + 24 = 72 \\]\n",
      "\n",
      "So, Natalia sold a total of 72 clips in April and May. The final answer is:\n",
      "\n",
      "\\[\n",
      "\\boxed{72}\n",
      "\\]\n",
      "========================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctness_reward_func(prompts=[dataset[0]['prompt']],\n",
    "                        completions=completions,\n",
    "                        answer=dataset[0]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"  \n",
    "    检查模型输出是否为有效整数，并根据结果给予奖励。\n",
    "\n",
    "    输入：\n",
    "    completions: 由模型回答组成的列表。\n",
    "\n",
    "    输出：\n",
    "    奖励分数列表。\n",
    "    \"\"\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `responses = [completion[0]['content'] for completion in completions]`: 提取模型回答的内容\n",
    "- `extracted_responses = [extract_xml_answer(r) for r in responses]`: 从模型回答中提取答案\n",
    "- `return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]`: 检查回答的答案是否为整数形式。如果是则给予奖励 0.5；否则不给予奖励。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_reward_func(completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 强格式检验\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"  \n",
    "    检查模型输出是否符合严格的格式要求。\n",
    "\n",
    "    输入：\n",
    "    completions: 由模型回答组成的列表。\n",
    "\n",
    "    输出：\n",
    "    奖励分数列表。\n",
    "    \"\"\"\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ` pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"`: 正则表达式，确保输出格式严格符合要求\n",
    "- `responses = [completion[0][\"content\"] for completion in completions]`: 从模型回答中提取问题答案\n",
    "- `matches = [re.match(pattern, r) for r in responses]`: 通过正则表达式检查每个回答是否符合格式\n",
    "- `return [0.5 if match else 0.0 for match in matches]`: 如果格式匹配，则给予奖励 0.5；否则不给予奖励"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strict_format_reward_func(completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 弱格式检验\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"  \n",
    "    检查模型的输出是否符合稍微宽松的格式要求。\n",
    "\n",
    "    输入：\n",
    "    completions: 由模型回答组成的列表。\n",
    "\n",
    "    输出：\n",
    "    奖励分数列表。\n",
    "    \"\"\"\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"`: 定义了一个稍微宽松的表达式\n",
    "- `responses = [completion[0][\"content\"] for completion in completions]`: 从模型回答中提取问题答案\n",
    "- `matches = [re.match(pattern, r) for r in responses]`: 检查每个回答是否符合此格式\n",
    "- `return [0.5 if match else 0.0 for match in matches]`: 如果格式匹配则给予奖励 0.5；否则不给予奖励"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_format_reward_func(completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标签计数检验\n",
    "def count_xml(text) -> float:\n",
    "    \"\"\"  \n",
    "    计算文本中标签的出现次数，本根据他们的位置和频率分配奖励。\n",
    "\n",
    "    输入：\n",
    "    text: 计数文本。\n",
    "\n",
    "    输出：\n",
    "    count: 标签总分。\n",
    "    \"\"\"\n",
    "    count = 0.0\n",
    "    if text.count(\"<reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `count = 0.0`: 初始化计数器\n",
    "- `if text.count(\"<reasoning>\\n\") == 1:`: 检查是否有且仅有一个`<reasoning>\\n`标签\n",
    "- `count += 0.125`: 条件满足则奖励 0.125\n",
    "- `if text.count(\"\\n</reasoning>\\n\") == 1:`: 检查是否有且仅有一个`\\n<reasoning>`标签\n",
    "- `count += 0.125`: 条件满足则奖励 0.125\n",
    "- `if text.count(\"\\n<answer>\\n\") == 1:`: 检查是否有且仅有一个`\\n<answer>\\n`标签\n",
    "- `count += 0.125`: 条件满足则奖励 0.125\n",
    "- `count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001`: 如果在标签`\\n<answer>\\n`之后存在多余的文本，按照文本长度扣除一些奖励\n",
    "- `if text.count(\"\\n</answer>\") == 1:`: 检查是否有且仅有一个`\\n</answer>`标签\n",
    "- `count += 0.125`: 条件满足则奖励 0.125\n",
    "- `count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001`: 如果存在多余文本则根据其长度扣除一些奖励"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_xml(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结构符合度检验\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"  \n",
    "    计算每个模型输出的 XML 结构符合度，并给予奖励。\n",
    "\n",
    "    输入：\n",
    "    completions: 由模型回答组成的列表。\n",
    "\n",
    "    输出：\n",
    "    奖励分数列表。\n",
    "    \"\"\"\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [count_xml(c) for c in contents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `contents = [completion[0][\"content\"] for completion in completions]`: 从模型回答中提取答案\n",
    "- `return [count_xml(c) for c in contents]`: 把`count_xml()`输出的结果作为奖励分数并返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmlcount_reward_func(completions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 GRPO 训练流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 训练参数设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从指定位置读取模型\n",
    "model_name = \"./Qwen2.5-0.5B-Instruct\"\n",
    "# 指定模型训练中间值的输出地址\n",
    "output_dir=\"outputs/Qwen-0.5B-GRPO\"\n",
    "# 指定项目运行的名字\n",
    "run_name=\"Qwen-0.5B-GRPO-gsm8k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    run_name=run_name,\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_generations=16,\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=200,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=100,\n",
    "    max_grad_norm=0.1,\n",
    "    log_on_each_node=False,\n",
    "    use_vllm=False,\n",
    "    vllm_gpu_memory_utilization=.3,\n",
    "    vllm_device=\"cuda:0\",\n",
    "    report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 加载模型和分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=None\n",
    ").to(\"cuda\")\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 设置网页端监控"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: wandb in /home/deepseek_tutorial/lib/python3.10/site-packages (0.19.8)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (2.10.6)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (2.22.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: platformdirs in /home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: pyyaml in /home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: setuptools in /home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (63.2.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (5.29.3)\n",
      "Requirement already satisfied: setproctitle in /home/deepseek_tutorial/lib/python3.10/site-packages (from wandb) (1.3.5)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/deepseek_tutorial/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/deepseek_tutorial/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/deepseek_tutorial/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/deepseek_tutorial/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/deepseek_tutorial/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/deepseek_tutorial/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/deepseek_tutorial/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/deepseek_tutorial/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/deepseek_tutorial/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzhangbihan999\u001b[0m (\u001b[33mzhangbihan999-sichuan-normal-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/Deepseek_tutorial/wandb/run-20250307_205324-kxt74971</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zhangbihan999-sichuan-normal-university/deepseek_tutorial/runs/kxt74971' target=\"_blank\">worldly-surf-2</a></strong> to <a href='https://wandb.ai/zhangbihan999-sichuan-normal-university/deepseek_tutorial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zhangbihan999-sichuan-normal-university/deepseek_tutorial' target=\"_blank\">https://wandb.ai/zhangbihan999-sichuan-normal-university/deepseek_tutorial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zhangbihan999-sichuan-normal-university/deepseek_tutorial/runs/kxt74971' target=\"_blank\">https://wandb.ai/zhangbihan999-sichuan-normal-university/deepseek_tutorial/runs/kxt74971</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/zhangbihan999-sichuan-normal-university/deepseek_tutorial/runs/kxt74971?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f07814b7160>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"b45a51a2aa3d2984d2ac2089d28e9c6c94538a3a\")\n",
    "wandb.init(project=\"deepseek_tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 开始模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f79b8ceeb00>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f78bc803100, raw_cell=\"from trl import GRPOTrainer\n",
      "\n",
      "trainer = GRPOTrainer..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2Bconnect.cqa1.seetacloud.com/home/Deepseek_tutorial/rewrite.ipynb#Y113sdnNjb2RlLXJlbW90ZQ%3D%3D>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:570\u001b[0m, in \u001b[0;36m_WandbInit._resume_backend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39minterface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresuming backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/interface/interface.py:777\u001b[0m, in \u001b[0;36mInterfaceBase.publish_resume\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    776\u001b[0m     resume \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mResumeRequest()\n\u001b[0;32m--> 777\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:293\u001b[0m, in \u001b[0;36mInterfaceShared._publish_resume\u001b[0;34m(self, resume)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb\u001b[38;5;241m.\u001b[39mResumeRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(resume\u001b[38;5;241m=\u001b[39mresume)\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py:39\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:174\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    172\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrequest_id \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mmailbox_slot\n\u001b[1;32m    173\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:154\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: spb\u001b[38;5;241m.\u001b[39mServerRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:151\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    149\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtrl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GRPOTrainer\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m GRPOTrainer(\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      5\u001b[0m     processing_class\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 每隔指定步将模型权重保存到指定文件夹\u001b[39;00m\n\u001b[1;32m     19\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(output_dir)\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/transformers/trainer.py:2455\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2453\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m   2454\u001b[0m grad_norm: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2455\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallback_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39meval_on_start:\n\u001b[1;32m   2458\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/transformers/trainer_callback.py:507\u001b[0m, in \u001b[0;36mCallbackHandler.on_train_begin\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mon_train_begin\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001b[1;32m    506\u001b[0m     control\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_event\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_train_begin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/transformers/trainer_callback.py:557\u001b[0m, in \u001b[0;36mCallbackHandler.call_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_event\u001b[39m(\u001b[38;5;28mself\u001b[39m, event, args, state, control, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 557\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m         \u001b[38;5;66;03m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[1;32m    570\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/transformers/integrations/integration_utils.py:916\u001b[0m, in \u001b[0;36mWandbCallback.on_train_begin\u001b[0;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m     args\u001b[38;5;241m.\u001b[39mrun_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialized:\n\u001b[0;32m--> 916\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/transformers/integrations/integration_utils.py:848\u001b[0m, in \u001b[0;36mWandbCallback.setup\u001b[0;34m(self, args, state, model, **kwargs)\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[1;32m    844\u001b[0m         project\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWANDB_PROJECT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuggingface\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    845\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_args,\n\u001b[1;32m    846\u001b[0m     )\n\u001b[1;32m    847\u001b[0m \u001b[38;5;66;03m# add config parameters (run may have been created manually)\u001b[39;00m\n\u001b[0;32m--> 848\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_val_change\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;66;03m# define default x-axis (for latest wandb versions)\u001b[39;00m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wandb, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefine_metric\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/wandb_config.py:189\u001b[0m, in \u001b[0;36mConfig.update\u001b[0;34m(self, d, allow_val_change)\u001b[0m\n\u001b[1;32m    187\u001b[0m sanitized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update(d, allow_val_change)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msanitized\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:401\u001b[0m, in \u001b[0;36m_run_decorator._noop_on_finish.<locals>.decorator_fn.<locals>.wrapper_fn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;28mtype\u001b[39m[Run], \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_finished\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m     default_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is finished. The call to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be ignored. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease make sure that you are using an active run.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    406\u001b[0m     )\n\u001b[1;32m    407\u001b[0m     resolved_message \u001b[38;5;241m=\u001b[39m message \u001b[38;5;129;01mor\u001b[39;00m default_message\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:1263\u001b[0m, in \u001b[0;36mRun._config_callback\u001b[0;34m(self, key, val, data)\u001b[0m\n\u001b[1;32m   1261\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_cb \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39minterface:\n\u001b[0;32m-> 1263\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/interface/interface.py:202\u001b[0m, in \u001b[0;36mInterfaceBase.publish_config\u001b[0;34m(self, data, key, val)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpublish_config\u001b[39m(\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    196\u001b[0m     data: Optional[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    197\u001b[0m     key: Optional[Union[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    198\u001b[0m     val: Optional[Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    199\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     cfg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_config(data\u001b[38;5;241m=\u001b[39mdata, key\u001b[38;5;241m=\u001b[39mkey, val\u001b[38;5;241m=\u001b[39mval)\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:301\u001b[0m, in \u001b[0;36mInterfaceShared._publish_config\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish_config\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg: pb\u001b[38;5;241m.\u001b[39mConfigRecord) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    300\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_record(config\u001b[38;5;241m=\u001b[39mcfg)\n\u001b[0;32m--> 301\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py:39\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:174\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    172\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrequest_id \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mmailbox_slot\n\u001b[1;32m    173\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:154\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: spb\u001b[38;5;241m.\u001b[39mServerRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:151\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    149\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f79b8ceeb00>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f78bc802620, execution_count=38 error_before_exec=None error_in_exec=[Errno 32] Broken pipe info=<ExecutionInfo object at 7f78bc803100, raw_cell=\"from trl import GRPOTrainer\n",
      "\n",
      "trainer = GRPOTrainer..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2Bconnect.cqa1.seetacloud.com/home/Deepseek_tutorial/rewrite.ipynb#Y113sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:565\u001b[0m, in \u001b[0;36m_WandbInit._pause_backend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39minterface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpausing backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/interface/interface.py:769\u001b[0m, in \u001b[0;36mInterfaceBase.publish_pause\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpublish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    768\u001b[0m     pause \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mPauseRequest()\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpause\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:289\u001b[0m, in \u001b[0;36mInterfaceShared._publish_pause\u001b[0;34m(self, pause)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m, pause: pb\u001b[38;5;241m.\u001b[39mPauseRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(pause\u001b[38;5;241m=\u001b[39mpause)\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py:39\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:174\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    172\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrequest_id \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mmailbox_slot\n\u001b[1;32m    173\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:154\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: spb\u001b[38;5;241m.\u001b[39mServerRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:151\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    149\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/deepseek_tutorial/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "from trl import GRPOTrainer\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        xmlcount_reward_func,\n",
    "        soft_format_reward_func,\n",
    "        strict_format_reward_func,\n",
    "        int_reward_func,\n",
    "        correctness_reward_func],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 每隔指定步将模型权重保存到指定文件夹\n",
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 运行模型并查看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpo_model_name = \"./outputs/Qwen-0.5B-GRPO\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    grpo_model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Joy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够看出，对比此前原始模型的状态，现在的模型已经能够顺利产生思考过程，且严格按照规定格式回答问题。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
